# Airflow Orchestration System (CRA-284)
# E7 - Viral Learning Flywheel - Apache Airflow deployment for viral content orchestration
{{- if .Values.airflow.enabled }}
---
apiVersion: v1
kind: Secret
metadata:
  name: airflow-secret
  labels:
{{- include "threads.labels" . | nindent 4 }}
  annotations:
    reloader.stakater.com/match: "true"  # Auto-reload on secret changes
type: Opaque
data:
  # Base64 encoded PostgreSQL connection string for Airflow metadata DB
  airflow-database-url: {{ printf "postgresql://postgres:%s@postgres:5432/airflow" .Values.postgres.auth.postgresPassword | b64enc }}
  # Fernet key for encrypting passwords in Airflow metadata DB - MUST be 32 bytes base64 encoded
  fernet-key: {{ .Values.airflow.fernetKey | required "airflow.fernetKey is required" | b64enc }}
  # Webserver secret key for Flask sessions - MUST be unique per deployment
  webserver-secret-key: {{ .Values.airflow.webserverSecretKey | required "airflow.webserverSecretKey is required" | b64enc }}
  # Admin password for initial user creation
  admin-password: {{ .Values.airflow.adminPassword | required "airflow.adminPassword is required" | b64enc }}

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-config
  labels:
{{- include "threads.labels" . | nindent 4 }}
data:
  airflow.cfg: |
    [core]
    executor = KubernetesExecutor
    # Database connection will be set via environment variable for security
    load_examples = False
    dags_are_paused_at_creation = False
    max_active_runs_per_dag = {{ .Values.airflow.maxActiveRuns | default 3 }}
    parallelism = {{ .Values.airflow.parallelism | default 32 }}
    max_active_tasks_per_dag = {{ .Values.airflow.maxActiveTasks | default 16 }}
    # Enable task retries and improve error handling
    default_task_retries = {{ .Values.airflow.defaultTaskRetries | default 2 }}
    # Improve performance with connection pooling
    sql_alchemy_pool_size = {{ .Values.airflow.sqlAlchemyPoolSize | default 10 }}
    sql_alchemy_max_overflow = {{ .Values.airflow.sqlAlchemyMaxOverflow | default 20 }}
    sql_alchemy_pool_recycle = 3600
    
    [webserver]
    base_url = {{ .Values.airflow.webserver.baseUrl | default "http://localhost:8080" }}
    web_server_port = 8080
    web_server_host = 0.0.0.0
    # Secret key will be set via environment variable
    expose_config = {{ .Values.airflow.webserver.exposeConfig | default "False" }}
    # Enable authentication for production security
    authenticate = {{ .Values.airflow.webserver.authenticate | default "True" }}
    auth_backend = {{ .Values.airflow.webserver.authBackend | default "airflow.api.auth.backend.basic_auth" }}
    # Session timeout for security
    session_timeout_minutes = {{ .Values.airflow.webserver.sessionTimeout | default 30 }}
    # Rate limiting
    web_server_worker_timeout = {{ .Values.airflow.webserver.workerTimeout | default 120 }}
    
    [scheduler]
    dag_dir_list_interval = 300
    child_process_timeout = 600
    scheduler_heartbeat_sec = 5
    num_runs = -1
    processor_poll_interval = 1
    min_file_process_interval = 30
    dag_file_processor_timeout = 50
    max_threads = {{ .Values.airflow.schedulerThreads | default 2 }}
    
    [kubernetes]
    namespace = {{ .Release.Namespace }}
    airflow_configmap = airflow-config
    worker_container_repository = {{ tpl .Values.airflow.image.repository $ }}
    worker_container_tag = {{ tpl .Values.airflow.image.tag $ }}
    worker_container_image_pull_policy = {{ .Values.airflow.image.pullPolicy | default "IfNotPresent" }}
    delete_worker_pods = True
    delete_worker_pods_on_failure = False
    
    [api]
    auth_backend = airflow.api.auth.backend.default
    enable_experimental_api = True
    
    [logging]
    remote_logging = False
    logging_level = INFO
    fab_logging_level = WARN
    
    [metrics]
    statsd_on = {{ .Values.airflow.metrics.statsd.enabled | default "True" }}
    statsd_host = {{ .Values.airflow.metrics.statsd.host | default "prometheus-statsd-exporter" }}
    statsd_port = {{ .Values.airflow.metrics.statsd.port | default 9125 }}
    statsd_prefix = {{ .Values.airflow.metrics.statsd.prefix | default "airflow" }}
    # Enable Prometheus metrics
    metrics_allow_list = {{ .Values.airflow.metrics.allowList | default "airflow.dag_processing.last_runtime.*,airflow.dag_processing.import_errors,airflow.dag.*.*.duration,airflow.dag.*.*.success,airflow.dag.*.*.failure" }}

---
# Airflow Database Initialization Job
apiVersion: batch/v1
kind: Job
metadata:
  name: airflow-db-init
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-db-init
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  backoffLimit: 3
  activeDeadlineSeconds: 300
  template:
    metadata:
      labels:
{{- include "threads.labels" . | nindent 8 }}
        component: airflow-db-init
    spec:
      restartPolicy: OnFailure
      serviceAccountName: airflow
      securityContext:
        runAsNonRoot: true
        runAsUser: 50000
        runAsGroup: 50000
        fsGroup: 50000
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: airflow-db-init
        image: "{{ tpl .Values.airflow.image.repository $ }}:{{ tpl .Values.airflow.image.tag $ }}"
        imagePullPolicy: {{ .Values.airflow.image.pullPolicy | default "IfNotPresent" }}
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 50000
          runAsGroup: 50000
          capabilities:
            drop:
            - ALL
        command:
        - /bin/bash
        - -c
        - |
          set -e
          export HOME=/tmp
          echo "Waiting for PostgreSQL..."
          timeout=60
          while ! pg_isready -h postgres -p 5432 -U postgres; do
            sleep 2
            timeout=$((timeout-2))
            if [ $timeout -le 0 ]; then
              echo "Timeout waiting for PostgreSQL"
              exit 1
            fi
          done
          echo "Creating Airflow database if not exists..."
          PGPASSWORD={{ .Values.postgres.auth.postgresPassword }} psql -h postgres -U postgres -c "CREATE DATABASE airflow;" || true
          echo "Initializing Airflow database..."
          airflow db init
          echo "Creating admin user..."
          airflow users create --username {{ .Values.airflow.adminUsername | default "admin" }} --firstname Admin --lastname User --role Admin --email {{ .Values.airflow.adminEmail | default "admin@threads-agent.com" }} --password "$AIRFLOW_ADMIN_PASSWORD" || echo "User already exists"
        env:
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: airflow-database-url
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        - name: AIRFLOW_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: admin-password
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        volumeMounts:
        - name: airflow-config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
        - name: tmp-volume
          mountPath: /tmp
        - name: home-volume
          mountPath: /home/airflow
      volumes:
      - name: airflow-config
        configMap:
          name: airflow-config
      - name: tmp-volume
        emptyDir: {}
      - name: home-volume
        emptyDir: {}

---
# Airflow Webserver Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-webserver
spec:
  replicas: {{ .Values.airflow.webserver.replicas | default 1 }}
  selector:
    matchLabels:
      app: airflow-webserver
      component: airflow-webserver
  template:
    metadata:
      labels:
        app: airflow-webserver
        component: airflow-webserver
{{- include "threads.labels" . | nindent 8 }}
    spec:
      containers:
      - name: airflow-webserver
        image: "{{ tpl .Values.airflow.image.repository $ }}:{{ tpl .Values.airflow.image.tag $ }}"
        imagePullPolicy: {{ .Values.airflow.image.pullPolicy | default "IfNotPresent" }}
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "Starting Airflow webserver..."
          airflow webserver
        ports:
        - name: http
          containerPort: 8080
        env:
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: airflow-database-url
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        - name: AIRFLOW__WEBSERVER__SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: webserver-secret-key
        # Service URLs for DAG integration
        - name: ORCHESTRATOR_URL
          value: "http://orchestrator:8080"
        - name: VIRAL_SCRAPER_URL  
          value: "http://viral-scraper:8080"
        - name: VIRAL_ENGINE_URL
          value: "http://viral-engine:8080"
        - name: VIRAL_PATTERN_ENGINE_URL
          value: "http://viral-pattern-engine:8080"
        - name: PERSONA_RUNTIME_URL
          value: "http://persona-runtime:8080"
        envFrom:
        - secretRef:
            name: openai-secret
        volumeMounts:
        - name: airflow-config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
          readOnly: true
        - name: dags-volume
          mountPath: /opt/airflow/dags
          readOnly: true
        - name: tmp-volume
          mountPath: /tmp
        - name: home-volume
          mountPath: /home/airflow
        - name: logs-volume
          mountPath: /opt/airflow/logs
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
        resources:
{{- toYaml .Values.airflow.webserver.resources | nindent 10 }}
      volumes:
      - name: airflow-config
        configMap:
          name: airflow-config
          defaultMode: 0444
      - name: dags-volume
        configMap:
          name: airflow-dags
          defaultMode: 0444
      - name: tmp-volume
        emptyDir:
          sizeLimit: 1Gi
      - name: home-volume
        emptyDir:
          sizeLimit: 1Gi
      - name: logs-volume
        emptyDir:
          sizeLimit: 5Gi

---
# Airflow Scheduler Deployment  
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-scheduler
spec:
  replicas: {{ .Values.airflow.scheduler.replicas | default 1 }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: airflow-scheduler
      component: airflow-scheduler
  template:
    metadata:
      labels:
        app: airflow-scheduler
        component: airflow-scheduler
{{- include "threads.labels" . | nindent 8 }}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8793"
    spec:
      serviceAccountName: airflow
      securityContext:
        runAsNonRoot: true
        runAsUser: 50000
        runAsGroup: 50000
        fsGroup: 50000
        seccompProfile:
          type: RuntimeDefault
      {{- if gt (int (.Values.airflow.scheduler.replicas | default 1)) 1 }}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: component
                  operator: In
                  values:
                  - airflow-scheduler
              topologyKey: kubernetes.io/hostname
      {{- end }}
      containers:
      - name: airflow-scheduler
        image: "{{ tpl .Values.airflow.image.repository $ }}:{{ tpl .Values.airflow.image.tag $ }}"
        imagePullPolicy: {{ .Values.airflow.image.pullPolicy | default "IfNotPresent" }}
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 50000
          runAsGroup: 50000
          capabilities:
            drop:
            - ALL
        command:
        - /bin/bash
        - -c
        - |
          set -e
          export HOME=/tmp
          echo "Starting Airflow scheduler..."
          airflow scheduler
        ports:
        - name: metrics
          containerPort: 8793
          protocol: TCP
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              pgrep -f "airflow scheduler" > /dev/null
          initialDelaySeconds: 120
          periodSeconds: 60
          timeoutSeconds: 10
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              pgrep -f "airflow scheduler" > /dev/null
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        env:
        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: airflow-database-url
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        # Service URLs for DAG operations
        - name: ORCHESTRATOR_URL
          value: "http://orchestrator:8080"
        - name: VIRAL_SCRAPER_URL
          value: "http://viral-scraper:8080"
        - name: VIRAL_ENGINE_URL
          value: "http://viral-engine:8080"
        - name: VIRAL_PATTERN_ENGINE_URL
          value: "http://viral-pattern-engine:8080"
        - name: PERSONA_RUNTIME_URL
          value: "http://persona-runtime:8080"
        envFrom:
        - secretRef:
            name: openai-secret
        volumeMounts:
        - name: airflow-config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
          readOnly: true
        - name: dags-volume
          mountPath: /opt/airflow/dags
          readOnly: true
        - name: tmp-volume
          mountPath: /tmp
        - name: home-volume
          mountPath: /home/airflow
        - name: logs-volume
          mountPath: /opt/airflow/logs
        resources:
{{- toYaml .Values.airflow.scheduler.resources | nindent 10 }}
      volumes:
      - name: airflow-config
        configMap:
          name: airflow-config
          defaultMode: 0444
      - name: dags-volume
        configMap:
          name: airflow-dags
          defaultMode: 0444
      - name: tmp-volume
        emptyDir:
          sizeLimit: 1Gi
      - name: home-volume
        emptyDir:
          sizeLimit: 1Gi
      - name: logs-volume
        emptyDir:
          sizeLimit: 5Gi

---
# Airflow Webserver Service
apiVersion: v1
kind: Service
metadata:
  name: airflow-webserver
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-webserver
spec:
  type: {{ .Values.airflow.service.type | default "ClusterIP" }}
  selector:
    app: airflow-webserver
    component: airflow-webserver
  ports:
  - name: http
    port: {{ .Values.airflow.service.port | default 8080 }}
    targetPort: 8080

---
# Service Account for Airflow (Kubernetes Executor)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: airflow
  labels:
{{- include "threads.labels" . | nindent 4 }}

---
# ClusterRole for Airflow Kubernetes Executor
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: airflow-kubernetes-executor
  labels:
{{- include "threads.labels" . | nindent 4 }}
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["create", "get", "list", "watch", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "list", "watch"]

---
# ClusterRoleBinding for Airflow
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: airflow-kubernetes-executor
  labels:
{{- include "threads.labels" . | nindent 4 }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: airflow-kubernetes-executor
subjects:
- kind: ServiceAccount
  name: airflow
  namespace: {{ .Release.Namespace }}

---
# Network Policy for Airflow Components
{{- if .Values.airflow.networkPolicy.enabled }}
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: airflow-network-policy
  labels:
{{- include "threads.labels" . | nindent 4 }}
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: {{ include "threads.name" . }}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Allow webserver traffic
  - from:
    - namespaceSelector:
        matchLabels:
          name: {{ .Release.Namespace }}
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: {{ include "threads.name" . }}
    ports:
    - protocol: TCP
      port: 8080
  # Allow scheduler metrics
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    - podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 8793
  egress:
  # Allow DNS resolution
  - to: []
    ports:
    - protocol: UDP
      port: 53
  # Allow PostgreSQL access
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
  # Allow service communication
  - to:
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: {{ include "threads.name" . }}
    ports:
    - protocol: TCP
      port: 8080
  # Allow external API calls (OpenAI, etc.)
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 80
{{- end }}

---
# Pod Disruption Budget for Webserver
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: airflow-webserver-pdb
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-webserver
spec:
  minAvailable: {{ .Values.airflow.webserver.pdb.minAvailable | default 1 }}
  selector:
    matchLabels:
      app: airflow-webserver
      component: airflow-webserver

---
# Pod Disruption Budget for Scheduler
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: airflow-scheduler-pdb
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-scheduler
spec:
  minAvailable: {{ .Values.airflow.scheduler.pdb.minAvailable | default 1 }}
  selector:
    matchLabels:
      app: airflow-scheduler
      component: airflow-scheduler

---
# ServiceMonitor for Prometheus
{{- if .Values.monitoring.prometheus.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: airflow-webserver
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-webserver
spec:
  selector:
    matchLabels:
      app: airflow-webserver
      component: airflow-webserver
  endpoints:
  - port: http
    path: /admin/metrics
    interval: {{ .Values.airflow.metrics.scrapeInterval | default "30s" }}
    scrapeTimeout: {{ .Values.airflow.metrics.scrapeTimeout | default "10s" }}

---
# ServiceMonitor for Scheduler Metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: airflow-scheduler
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-scheduler
spec:
  selector:
    matchLabels:
      app: airflow-scheduler
      component: airflow-scheduler
  endpoints:
  - port: metrics
    path: /metrics
    interval: {{ .Values.airflow.metrics.scrapeInterval | default "30s" }}
    scrapeTimeout: {{ .Values.airflow.metrics.scrapeTimeout | default "10s" }}
{{- end }}

---
# Horizontal Pod Autoscaler for Webserver
{{- if .Values.airflow.webserver.autoscaling.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: airflow-webserver-hpa
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-webserver
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: airflow-webserver
  minReplicas: {{ .Values.airflow.webserver.autoscaling.minReplicas | default 1 }}
  maxReplicas: {{ .Values.airflow.webserver.autoscaling.maxReplicas | default 5 }}
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{ .Values.airflow.webserver.autoscaling.targetCPUUtilization | default 70 }}
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: {{ .Values.airflow.webserver.autoscaling.targetMemoryUtilization | default 80 }}
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
{{- end }}

---
# Service for Scheduler Metrics
apiVersion: v1
kind: Service
metadata:
  name: airflow-scheduler-metrics
  labels:
{{- include "threads.labels" . | nindent 4 }}
    component: airflow-scheduler
spec:
  type: ClusterIP
  selector:
    app: airflow-scheduler
    component: airflow-scheduler
  ports:
  - name: metrics
    port: 8793
    targetPort: 8793

{{- end }}