{{- if .Values.vllmService.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "threads-agent.fullname" . }}-vllm-service
  labels:
    {{- include "threads-agent.labels" . | nindent 4 }}
    component: vllm-service
    tier: ai-inference
spec:
  replicas: {{ .Values.vllmService.replicaCount }}
  strategy:
    type: {{ .Values.vllmService.updateStrategy.type }}
    {{- if eq .Values.vllmService.updateStrategy.type "RollingUpdate" }}
    rollingUpdate:
      maxSurge: {{ .Values.vllmService.updateStrategy.maxSurge }}
      maxUnavailable: {{ .Values.vllmService.updateStrategy.maxUnavailable }}
    {{- end }}
  selector:
    matchLabels:
      {{- include "threads-agent.selectorLabels" . | nindent 6 }}
      component: vllm-service
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8090"
        prometheus.io/path: "/metrics"
        checksum/config: {{ include (print $.Template.BasePath "/secret-openai.yaml") . | sha256sum }}
      labels:
        {{- include "threads-agent.selectorLabels" . | nindent 8 }}
        component: vllm-service
        tier: ai-inference
    spec:
      {{- with .Values.vllmService.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "threads-agent.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.vllmService.podSecurityContext | nindent 8 }}
      # Apple Silicon node affinity for optimal performance
      {{- if .Values.vllmService.nodeAffinity.enabled }}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values:
                - arm64
          - weight: 80
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - m1.large
                - m1.xlarge
                - m2.large
                - m2.xlarge
          {{- if .Values.vllmService.nodeAffinity.requiredNodeSelectorTerms }}
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            {{- toYaml .Values.vllmService.nodeAffinity.requiredNodeSelectorTerms | nindent 12 }}
          {{- end }}
      {{- end }}
      containers:
      - name: vllm-service
        securityContext:
          {{- toYaml .Values.vllmService.securityContext | nindent 10 }}
        image: "{{ .Values.vllmService.image.repository }}:{{ .Values.vllmService.image.tag | default .Chart.AppVersion }}"
        imagePullPolicy: {{ .Values.vllmService.image.pullPolicy }}
        ports:
        - name: http
          containerPort: 8090
          protocol: TCP
        - name: metrics
          containerPort: 8090
          protocol: TCP
        # Enhanced health checks for <50ms performance validation
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: {{ .Values.vllmService.livenessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.vllmService.livenessProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.vllmService.livenessProbe.timeoutSeconds }}
          successThreshold: {{ .Values.vllmService.livenessProbe.successThreshold }}
          failureThreshold: {{ .Values.vllmService.livenessProbe.failureThreshold }}
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: {{ .Values.vllmService.readinessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.vllmService.readinessProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.vllmService.readinessProbe.timeoutSeconds }}
          successThreshold: {{ .Values.vllmService.readinessProbe.successThreshold }}
          failureThreshold: {{ .Values.vllmService.readinessProbe.failureThreshold }}
        # Startup probe for model loading with extended timeout
        startupProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: {{ .Values.vllmService.startupProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.vllmService.startupProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.vllmService.startupProbe.timeoutSeconds }}
          failureThreshold: {{ .Values.vllmService.startupProbe.failureThreshold }}
        # Resource limits optimized for Apple Silicon and LLM inference
        resources:
          {{- toYaml .Values.vllmService.resources | nindent 10 }}
        env:
        # Model configuration
        - name: VLLM_MODEL
          value: {{ .Values.vllmService.model.name | quote }}
        - name: VLLM_MODEL_CACHE_DIR
          value: {{ .Values.vllmService.model.cacheDir | quote }}
        - name: VLLM_TENSOR_PARALLEL_SIZE
          value: {{ .Values.vllmService.model.tensorParallelSize | quote }}
        - name: VLLM_GPU_MEMORY_UTILIZATION
          value: {{ .Values.vllmService.model.gpuMemoryUtilization | quote }}
        
        # Performance optimization
        - name: VLLM_DISABLE_LOG_STATS
          value: {{ .Values.vllmService.performance.disableLogStats | quote }}
        - name: VLLM_MAX_NUM_SEQS
          value: {{ .Values.vllmService.performance.maxNumSeqs | quote }}
        - name: VLLM_MAX_NUM_BATCHED_TOKENS
          value: {{ .Values.vllmService.performance.maxNumBatchedTokens | quote }}
        
        # Apple Silicon optimizations
        - name: FORCE_CPU
          value: {{ .Values.vllmService.appleSilicon.forceCpu | quote }}
        - name: PYTORCH_MPS_HIGH_WATERMARK_RATIO
          value: {{ .Values.vllmService.appleSilicon.mpsHighWatermarkRatio | quote }}
        - name: MALLOC_ARENA_MAX
          value: {{ .Values.vllmService.appleSilicon.mallocArenaMax | quote }}
        - name: PYTHONMALLOC
          value: {{ .Values.vllmService.appleSilicon.pythonMalloc | quote }}
        
        # Integration endpoints
        - name: ORCHESTRATOR_URL
          value: "http://{{ include "threads-agent.fullname" . }}-orchestrator:8080"
        - name: RAG_PIPELINE_URL
          value: "http://{{ include "threads-agent.fullname" . }}-rag-pipeline:8085"
        - name: PROMETHEUS_GATEWAY_URL
          value: "http://{{ include "threads-agent.fullname" . }}-prometheus:9090"
        
        # OpenAI API compatibility
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: {{ include "threads-agent.fullname" . }}-openai-secret
              key: openai-api-key
        
        # Database configuration
        - name: DATABASE_URL
          value: {{ include "threads.postgres.dsn" . | quote }}
        
        # Redis for caching
        - name: REDIS_URL
          value: "redis://{{ include "threads-agent.fullname" . }}-redis:6379/{{ .Values.vllmService.redis.database }}"
        
        # Performance monitoring
        - name: LATENCY_TARGET_MS
          value: {{ .Values.vllmService.monitoring.latencyTargetMs | quote }}
        - name: COST_SAVINGS_TARGET_PERCENT
          value: {{ .Values.vllmService.monitoring.costSavingsTargetPercent | quote }}
        
        volumeMounts:
        - name: model-cache
          mountPath: {{ .Values.vllmService.model.cacheDir }}
        {{- if .Values.vllmService.persistence.logs.enabled }}
        - name: logs
          mountPath: /app/logs
        {{- end }}
        {{- with .Values.vllmService.volumeMounts }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      volumes:
      - name: model-cache
        {{- if .Values.vllmService.persistence.modelCache.enabled }}
        persistentVolumeClaim:
          claimName: {{ include "threads-agent.fullname" . }}-vllm-model-cache
        {{- else }}
        emptyDir:
          sizeLimit: {{ .Values.vllmService.persistence.modelCache.size }}
        {{- end }}
      {{- if .Values.vllmService.persistence.logs.enabled }}
      - name: logs
        persistentVolumeClaim:
          claimName: {{ include "threads-agent.fullname" . }}-vllm-logs
      {{- end }}
      {{- with .Values.vllmService.volumes }}
      {{- toYaml . | nindent 6 }}
      {{- end }}
      {{- with .Values.vllmService.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.vllmService.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
---
{{- if .Values.vllmService.persistence.modelCache.enabled }}
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ include "threads-agent.fullname" . }}-vllm-model-cache
  labels:
    {{- include "threads-agent.labels" . | nindent 4 }}
    component: vllm-service
spec:
  accessModes:
    - {{ .Values.vllmService.persistence.modelCache.accessMode }}
  resources:
    requests:
      storage: {{ .Values.vllmService.persistence.modelCache.size }}
  {{- if .Values.vllmService.persistence.modelCache.storageClass }}
  storageClassName: {{ .Values.vllmService.persistence.modelCache.storageClass }}
  {{- end }}
{{- end }}
---
{{- if .Values.vllmService.persistence.logs.enabled }}
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ include "threads-agent.fullname" . }}-vllm-logs
  labels:
    {{- include "threads-agent.labels" . | nindent 4 }}
    component: vllm-service
spec:
  accessModes:
    - {{ .Values.vllmService.persistence.logs.accessMode }}
  resources:
    requests:
      storage: {{ .Values.vllmService.persistence.logs.size }}
  {{- if .Values.vllmService.persistence.logs.storageClass }}
  storageClassName: {{ .Values.vllmService.persistence.logs.storageClass }}
  {{- end }}
{{- end }}
---
apiVersion: v1
kind: Service
metadata:
  name: {{ include "threads-agent.fullname" . }}-vllm-service
  labels:
    {{- include "threads-agent.labels" . | nindent 4 }}
    component: vllm-service
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8090"
    prometheus.io/path: "/metrics"
spec:
  type: {{ .Values.vllmService.service.type }}
  ports:
  - port: {{ .Values.vllmService.service.port }}
    targetPort: http
    protocol: TCP
    name: http
  - port: {{ .Values.vllmService.service.metricsPort }}
    targetPort: metrics
    protocol: TCP
    name: metrics
  selector:
    {{- include "threads-agent.selectorLabels" . | nindent 4 }}
    component: vllm-service
{{- end }}