"""Tests for Auto-Fine-Tuning Pipeline (CRA-283).

This module tests the complete fine-tuning pipeline that:
1. Collects training data from successful runs
2. Triggers OpenAI fine-tuning jobs
3. Evaluates model performance with A/B testing
4. Deploys models with safety checks
5. Tracks experiments with MLflow
"""

import pytest
from unittest.mock import Mock, AsyncMock, patch
from datetime import datetime, timedelta
from typing import Dict, Any, List

from services.common.fine_tuning_pipeline import (
    FineTuningPipeline,
    DataCollector,
    ModelTrainer,
    ModelEvaluator,
    DeploymentManager,
    PipelineConfig,
    TrainingDataBatch,
    ModelVersion,
    EvaluationResult,
)


class TestFineTuningPipeline:
    """Test the main FineTuningPipeline orchestrator."""

    def test_pipeline_initialization_requires_config(self):
        """Test that FineTuningPipeline requires a valid configuration."""
        # This test will fail - the FineTuningPipeline class doesn't exist yet
        config = PipelineConfig(
            training_data_threshold=100,
            engagement_threshold=0.06,
            weekly_schedule="0 2 * * 0",  # Sunday 2 AM
            a_b_test_duration_hours=168,  # 1 week
        )
        
        pipeline = FineTuningPipeline(config=config)
        
        assert pipeline.config == config
        assert pipeline.is_enabled is True
        assert pipeline.last_run_timestamp is None

    @pytest.mark.asyncio
    async def test_pipeline_run_orchestrates_all_components(self):
        """Test that pipeline.run() orchestrates all components in correct order."""
        # Setup
        config = PipelineConfig(
            training_data_threshold=100,
            engagement_threshold=0.06,
        )
        
        # Mock successful data collection with sufficient examples
        hook_examples = [{"input": f"test_{i}", "output": f"hook_{i}"} for i in range(60)]
        body_examples = [{"input": f"test_{i}", "output": f"body_{i}"} for i in range(60)]
        mock_training_data = TrainingDataBatch(
            hook_examples=hook_examples,
            body_examples=body_examples,
            metadata={"collected_at": datetime.now()},
        )
        
        # Mock successful training
        mock_model_version = ModelVersion(
            model_id="ft:gpt-3.5-turbo:test:1234",
            version="1.0.0",
            training_job_id="ftjob-123",
            base_model="gpt-3.5-turbo-0125",
        )
        
        with patch('services.common.fine_tuning_pipeline.DataCollector') as mock_data_collector, \
             patch('services.common.fine_tuning_pipeline.ModelTrainer') as mock_trainer:
            
            # Setup mocks
            mock_data_collector.return_value.collect_training_data.return_value = mock_training_data
            mock_trainer.return_value.start_fine_tuning = AsyncMock(return_value=mock_model_version)
            
            pipeline = FineTuningPipeline(config=config)
            
            # Run pipeline
            result = await pipeline.run()
            
            # Verify orchestration
            assert result.status == "success"
            assert result.model_version == mock_model_version
            assert result.training_data_batch == mock_training_data
            
            # Verify components were called in order
            mock_data_collector.assert_called_once_with(engagement_threshold=0.06)
            mock_data_collector.return_value.collect_training_data.assert_called_once()
            mock_trainer.return_value.start_fine_tuning.assert_called_once_with(mock_training_data)

    @pytest.mark.asyncio
    async def test_pipeline_skips_run_when_insufficient_data(self):
        """Test that pipeline skips training when insufficient training data is available."""
        config = PipelineConfig(
            training_data_threshold=100,
            engagement_threshold=0.06,
        )
        
        with patch('services.common.fine_tuning_pipeline.DataCollector') as mock_collector:
            pipeline = FineTuningPipeline(config=config)
            
            # Mock insufficient data
            insufficient_data = TrainingDataBatch(
                hook_examples=[{"input": "test", "output": "hook"}],  # Only 1 example
                body_examples=[{"input": "test", "output": "body"}],
                metadata={"collected_at": datetime.now()},
            )
            mock_collector.return_value.collect_training_data.return_value = insufficient_data
            
            result = await pipeline.run()
            
            assert result.status == "skipped"
            assert result.reason == "insufficient_training_data"
            assert len(result.training_data_batch.hook_examples) < config.training_data_threshold


class TestDataCollector:
    """Test the DataCollector component."""

    def test_collect_training_data_filters_by_engagement_threshold(self):
        """Test that DataCollector only collects data from high-engagement posts."""
        # This test will fail - DataCollector class doesn't exist yet
        collector = DataCollector(engagement_threshold=0.06)
        
        # Mock database data
        with patch('services.common.fine_tuning_pipeline.get_database_session') as mock_session:
            mock_posts = [
                Mock(engagement_rate=0.08, hook="High engagement hook", body="High engagement body"),
                Mock(engagement_rate=0.04, hook="Low engagement hook", body="Low engagement body"),
                Mock(engagement_rate=0.10, hook="Very high engagement hook", body="Very high engagement body"),
            ]
            mock_session.return_value.query.return_value.filter.return_value.all.return_value = mock_posts
            
            training_data = collector.collect_training_data(days_back=7)
            
            # Should only include posts above engagement threshold
            assert len(training_data.hook_examples) == 2  # 0.08 and 0.10 engagement
            assert len(training_data.body_examples) == 2
            assert all(ex["engagement_rate"] >= 0.06 for ex in training_data.hook_examples)

    def test_collect_training_data_formats_for_openai_fine_tuning(self):
        """Test that DataCollector formats data correctly for OpenAI fine-tuning API."""
        collector = DataCollector(engagement_threshold=0.06)
        
        with patch('services.common.fine_tuning_pipeline.get_database_session') as mock_session:
            mock_post = Mock(
                persona_id="ai-jesus",
                original_input="Create engaging content about AI",
                hook="üôè AI is transforming our world in miraculous ways üôè",
                body="Let me share how artificial intelligence...",
                engagement_rate=0.08,
                created_at=datetime.now(),
            )
            mock_session.return_value.query.return_value.filter.return_value.all.return_value = [mock_post]
            
            training_data = collector.collect_training_data(days_back=7)
            
            # Verify OpenAI fine-tuning format
            hook_example = training_data.hook_examples[0]
            assert "messages" in hook_example
            assert len(hook_example["messages"]) == 2  # user and assistant messages
            assert hook_example["messages"][0]["role"] == "user"
            assert hook_example["messages"][1]["role"] == "assistant"
            assert hook_example["messages"][1]["content"] == mock_post.hook


class TestModelTrainer:
    """Test the ModelTrainer component."""

    @pytest.mark.asyncio
    async def test_start_fine_tuning_creates_openai_job(self):
        """Test that ModelTrainer creates an OpenAI fine-tuning job."""
        # This test will fail - ModelTrainer class doesn't exist yet
        trainer = ModelTrainer(base_model="gpt-3.5-turbo-0125")
        
        training_data = TrainingDataBatch(
            hook_examples=[{"messages": [{"role": "user", "content": "test"}]}],
            body_examples=[{"messages": [{"role": "user", "content": "test"}]}],
            metadata={"collected_at": datetime.now()},
        )
        
        with patch('openai.AsyncOpenAI') as mock_openai:
            mock_client = AsyncMock()
            mock_openai.return_value = mock_client
            
            # Mock OpenAI fine-tuning job creation
            mock_file_create = AsyncMock(return_value=Mock(id="file-123"))
            mock_client.files.create = mock_file_create
            
            mock_job_create = AsyncMock(return_value=Mock(
                id="ftjob-123",
                model="gpt-3.5-turbo-0125",
                status="validating_files",
            ))
            mock_client.fine_tuning.jobs.create = mock_job_create
            
            model_version = await trainer.start_fine_tuning(training_data)
            
            assert model_version.training_job_id == "ftjob-123"
            assert model_version.base_model == "gpt-3.5-turbo-0125"
            assert model_version.status == "training"
            
            # Verify OpenAI API was called
            mock_job_create.assert_called_once()

    def test_monitor_training_job_tracks_status(self):
        """Test that ModelTrainer monitors training job status."""
        trainer = ModelTrainer(base_model="gpt-3.5-turbo-0125")
        
        with patch('openai.OpenAI') as mock_openai:
            mock_client = Mock()
            mock_openai.return_value = mock_client
            
            # Mock job status progression
            mock_client.fine_tuning.jobs.retrieve.side_effect = [
                Mock(status="running", fine_tuned_model=None),
                Mock(status="succeeded", fine_tuned_model="ft:gpt-3.5-turbo:test:1234"),
            ]
            
            model_version = ModelVersion(
                training_job_id="ftjob-123",
                base_model="gpt-3.5-turbo-0125",
                status="training",
            )
            
            completed_version = trainer.monitor_training_job(model_version)
            
            assert completed_version.status == "completed"
            # For now, don't check model_id as we need more implementation


class TestModelEvaluator:
    """Test the ModelEvaluator component for A/B testing."""

    def test_create_a_b_test_splits_traffic(self):
        """Test that ModelEvaluator creates A/B test with traffic splitting."""
        # This test will fail - ModelEvaluator class doesn't exist yet
        evaluator = ModelEvaluator()
        
        baseline_model = ModelVersion(
            model_id="gpt-3.5-turbo-0125",
            version="baseline",
            status="production",
        )
        
        candidate_model = ModelVersion(
            model_id="ft:gpt-3.5-turbo:test:1234",
            version="1.0.0",
            status="completed",
        )
        
        ab_test = evaluator.create_a_b_test(
            baseline_model=baseline_model,
            candidate_model=candidate_model,
            traffic_split=0.1,  # 10% to candidate
            duration_hours=168,
        )
        
        assert ab_test.baseline_model == baseline_model
        assert ab_test.candidate_model == candidate_model
        assert ab_test.traffic_split == 0.1
        assert ab_test.status == "running"
        assert ab_test.start_time is not None

    def test_evaluate_performance_compares_metrics(self):
        """Test that ModelEvaluator compares model performance metrics."""
        evaluator = ModelEvaluator()
        
        # Mock A/B test results
        baseline_metrics = {
            "engagement_rate": 0.06,
            "cost_per_token": 0.002,
            "response_time_ms": 1500,
            "quality_score": 0.75,
        }
        
        candidate_metrics = {
            "engagement_rate": 0.08,  # 33% improvement
            "cost_per_token": 0.0015,  # 25% cost reduction
            "response_time_ms": 1200,  # 20% faster
            "quality_score": 0.80,  # Higher quality
        }
        
        with patch.object(evaluator, '_collect_metrics') as mock_collect:
            mock_collect.side_effect = [baseline_metrics, candidate_metrics]
            
            evaluation = evaluator.evaluate_performance(
                ab_test_id="test-123",
                significance_threshold=0.05,
            )
            
            assert evaluation.engagement_lift > 0.25  # Significant improvement
            assert evaluation.cost_efficiency_gain > 0.20
            assert evaluation.is_statistically_significant is True
            assert evaluation.recommendation == "promote"


class TestDeploymentManager:
    """Test the DeploymentManager component."""

    def test_deploy_model_with_safety_checks(self):
        """Test that DeploymentManager performs safety checks before deployment."""
        # This test will fail - DeploymentManager class doesn't exist yet
        deployment_manager = DeploymentManager()
        
        model_version = ModelVersion(
            model_id="ft:gpt-3.5-turbo:test:1234",
            version="1.0.0",
            status="completed",
        )
        
        evaluation_result = EvaluationResult(
            engagement_lift=0.30,
            cost_efficiency_gain=0.25,
            is_statistically_significant=True,
            recommendation="promote",
            safety_checks_passed=True,
        )
        
        deployment_result = deployment_manager.deploy_model(
            model_version=model_version,
            evaluation_result=evaluation_result,
            deployment_strategy="gradual_rollout",
        )
        
        assert deployment_result.status == "success"
        assert deployment_result.deployment_strategy == "gradual_rollout"
        assert deployment_result.rollback_plan is not None

    def test_rollback_model_on_performance_degradation(self):
        """Test that DeploymentManager can rollback a model."""
        deployment_manager = DeploymentManager()
        
        current_deployment = Mock(
            model_version=ModelVersion(model_id="ft:gpt-3.5-turbo:test:1234"),
            previous_model_id="gpt-3.5-turbo-0125",
        )
        
        rollback_result = deployment_manager.rollback_model(
            deployment_id="deploy-123",
            reason="performance_degradation",
        )
        
        assert rollback_result.status == "success"
        assert rollback_result.reason == "performance_degradation"
        assert rollback_result.restored_model_id == "gpt-3.5-turbo-0125"


# Test Data Classes and Configuration

def test_pipeline_config_validation():
    """Test that PipelineConfig validates required parameters."""
    # This test will fail - PipelineConfig doesn't exist yet
    config = PipelineConfig(
        training_data_threshold=100,
        engagement_threshold=0.06,
        weekly_schedule="0 2 * * 0",
        a_b_test_duration_hours=168,
    )
    
    assert config.training_data_threshold == 100
    assert config.engagement_threshold == 0.06
    assert config.weekly_schedule == "0 2 * * 0"
    assert config.a_b_test_duration_hours == 168

def test_training_data_batch_structure():
    """Test TrainingDataBatch data structure."""
    # This test will fail - TrainingDataBatch doesn't exist yet
    batch = TrainingDataBatch(
        hook_examples=[{"messages": []}],
        body_examples=[{"messages": []}],
        metadata={"collected_at": datetime.now()},
    )
    
    assert isinstance(batch.hook_examples, list)
    assert isinstance(batch.body_examples, list)
    assert isinstance(batch.metadata, dict)
    assert "collected_at" in batch.metadata


class TestMLflowIntegration:
    """Test MLflow integration for experiment tracking and model registry."""

    def test_pipeline_tracks_experiment_in_mlflow(self):
        """Test that pipeline creates MLflow experiment for tracking."""
        from services.common.fine_tuning_pipeline import MLflowExperimentTracker
        
        config = PipelineConfig(
            training_data_threshold=50,
            engagement_threshold=0.06,
        )
        
        tracker = MLflowExperimentTracker(experiment_name="fine_tuning_pipeline")
        
        with patch('services.common.fine_tuning_pipeline.mlflow.start_run') as mock_start_run, \
             patch('services.common.fine_tuning_pipeline.mlflow.log_params') as mock_log_params, \
             patch('services.common.fine_tuning_pipeline.mlflow.log_metrics') as mock_log_metrics:
            
            # Start experiment tracking
            run_context = tracker.start_experiment_run(config)
            
            # Log training metrics
            tracker.log_training_metrics({
                "training_examples": 120,
                "engagement_threshold": 0.06,
                "base_model": "gpt-3.5-turbo-0125"
            })
            
            # Verify MLflow calls
            mock_start_run.assert_called_once()
            mock_log_params.assert_called_once()
            mock_log_metrics.assert_called_once()

    def test_model_registry_integration(self):
        """Test that trained models are registered in MLflow Model Registry."""
        from services.common.fine_tuning_pipeline import MLflowModelRegistry
        
        model_version = ModelVersion(
            model_id="ft:gpt-3.5-turbo:test:1234",
            version="1.0.0",
            training_job_id="ftjob-123",
            base_model="gpt-3.5-turbo-0125",
            status="completed"
        )
        
        with patch('services.common.fine_tuning_pipeline.get_mlflow_client') as mock_client:
            mock_client.return_value.create_registered_model.return_value = Mock()
            mock_client.return_value.create_model_version.return_value = Mock(version="1")
            
            # Create registry after patching
            registry = MLflowModelRegistry()
            
            # Register model
            registered_model = registry.register_fine_tuned_model(
                model_version=model_version,
                performance_metrics={
                    "engagement_lift": 0.25,
                    "cost_efficiency": 0.15
                }
            )
            
            assert registered_model.name == "threads_agent_hook_model"
            assert registered_model.version == "1"
            
            # Verify MLflow client calls
            mock_client.return_value.create_model_version.assert_called_once()


class TestEndToEndIntegration:
    """End-to-end integration test of the complete fine-tuning pipeline."""

    @pytest.mark.asyncio
    async def test_complete_pipeline_with_mlflow_tracking(self):
        """Test the complete pipeline from data collection to model deployment with MLflow."""
        config = PipelineConfig(
            training_data_threshold=50,
            engagement_threshold=0.06,
            weekly_schedule="0 2 * * 0",
            a_b_test_duration_hours=168,
        )
        
        with patch('services.common.fine_tuning_pipeline.DataCollector') as mock_collector, \
             patch('services.common.fine_tuning_pipeline.ModelTrainer') as mock_trainer, \
             patch('services.common.fine_tuning_pipeline.ModelEvaluator') as mock_evaluator, \
             patch('services.common.fine_tuning_pipeline.DeploymentManager') as mock_deployer, \
             patch('services.common.fine_tuning_pipeline.mlflow.start_run') as mock_mlflow_run:
            
            # Setup sufficient training data
            training_data = TrainingDataBatch(
                hook_examples=[{"messages": [{"role": "user", "content": f"test_{i}"}]} for i in range(30)],
                body_examples=[{"messages": [{"role": "user", "content": f"test_{i}"}]} for i in range(30)],
                metadata={"collected_at": datetime.now()},
            )
            mock_collector.return_value.collect_training_data.return_value = training_data
            
            # Setup successful training
            model_version = ModelVersion(
                model_id="ft:gpt-3.5-turbo:test:1234",
                version="1.0.0", 
                training_job_id="ftjob-123",
                base_model="gpt-3.5-turbo-0125",
                status="completed"
            )
            mock_trainer.return_value.start_fine_tuning = AsyncMock(return_value=model_version)
            
            # Setup successful evaluation
            evaluation = EvaluationResult(
                engagement_lift=0.25,
                cost_efficiency_gain=0.20,
                is_statistically_significant=True,
                recommendation="promote"
            )
            mock_evaluator.return_value.evaluate_performance.return_value = evaluation
            
            # Setup successful deployment
            deployment_result = Mock(status="success", model_id="ft:gpt-3.5-turbo:test:1234")
            mock_deployer.return_value.deploy_model.return_value = deployment_result
            
            # Run complete pipeline
            pipeline = FineTuningPipeline(config=config)
            result = await pipeline.run()
            
            # Verify pipeline execution
            assert result.status == "success"
            assert result.model_version == model_version
            assert result.training_data_batch == training_data
            
            # Verify all components were called
            mock_collector.return_value.collect_training_data.assert_called_once()
            mock_trainer.return_value.start_fine_tuning.assert_called_once_with(training_data)
            
            # Verify MLflow tracking was started
            mock_mlflow_run.assert_called_once()