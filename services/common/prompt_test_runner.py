"""
PromptTestRunner - Automated testing framework for prompt templates.

Part of CRA-297 CI/CD Pipeline implementation. Provides automated testing capabilities
for prompt templates including validation, performance checks, and quality assurance.

This implementation follows strict TDD practices and integrates with the existing
MLflow Model Registry and PromptModel infrastructure.

Author: TDD Implementation for CRA-297
"""

import time
import json
import re
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum


class TestRunnerError(Exception):
    """Raised when PromptTestRunner encounters an error."""
    pass


@dataclass
class TestCase:
    """
    Represents a single test case for prompt template testing.
    
    Attributes:
        name: Unique identifier for the test case
        input_data: Dictionary of variables to pass to the prompt template
        expected_output: Expected output from the prompt template
        validation_rules: List of validation rules to apply
        metadata: Optional additional metadata for the test case
    """
    name: str
    input_data: Dict[str, Any]
    expected_output: str
    validation_rules: List[str]
    metadata: Optional[Dict[str, Any]] = field(default_factory=dict)


@dataclass
class TestResult:
    """
    Represents the result of running a single test case.
    
    Attributes:
        test_case_name: Name of the test case that was run
        passed: Whether the test passed all validation rules
        actual_output: Actual output generated by the prompt template
        expected_output: Expected output for comparison
        execution_time: Time taken to execute the prompt in seconds
        error: Error message if the test failed, None if passed
        validation_details: Detailed breakdown of validation results
    """
    test_case_name: str
    passed: bool
    actual_output: Optional[str]
    expected_output: str
    execution_time: float
    error: Optional[str] = None
    validation_details: Optional[Dict[str, Any]] = field(default_factory=dict)


@dataclass
class TestSuite:
    """
    Represents a collection of test cases to run against a prompt template.
    
    Attributes:
        name: Name of the test suite
        test_cases: List of TestCase objects to execute
        metadata: Optional additional metadata for the test suite
    """
    name: str
    test_cases: List[TestCase]
    metadata: Optional[Dict[str, Any]] = field(default_factory=dict)


class ValidationRule(Enum):
    """Enumeration of available validation rules."""
    EXACT_MATCH = "exact_match"
    CONTAINS = "contains"
    REGEX_MATCH = "regex_match"
    LENGTH_RANGE = "length_range"
    MAX_EXECUTION_TIME = "max_execution_time"
    # Custom semantic validation rules
    CONTAINS_NAME = "contains_name"
    GREETING_FORMAT = "greeting_format"
    HANDLES_EMPTY_INPUT = "handles_empty_input"


class PromptTestRunner:
    """
    Automated testing framework for prompt templates.
    
    Provides comprehensive testing capabilities including:
    - Template output validation
    - Performance measurement
    - Multiple validation rule support
    - Detailed reporting and analytics
    
    Integrates with existing PromptModel infrastructure for seamless CI/CD workflows.
    """
    
    def __init__(self, prompt_model, test_suite: Optional[TestSuite] = None):
        """
        Initialize PromptTestRunner.
        
        Args:
            prompt_model: PromptModel instance to test
            test_suite: Optional TestSuite to run
            
        Raises:
            TestRunnerError: If prompt_model is None
        """
        if prompt_model is None:
            raise TestRunnerError("Prompt model cannot be None")
            
        self.prompt_model = prompt_model
        self.test_suite = test_suite
        self.test_results: List[TestResult] = []
        self.is_running = False
    
    def run_test_case(self, test_case: TestCase) -> TestResult:
        """
        Run a single test case against the prompt model.
        
        Args:
            test_case: TestCase to execute
            
        Returns:
            TestResult with execution details and validation results
            
        Raises:
            TestRunnerError: If validation rule is unknown
        """
        start_time = time.time()
        actual_output = None
        error = None
        
        try:
            # Execute the prompt template
            actual_output = self.prompt_model.render(**test_case.input_data)
            
            # Run validation rules
            validation_passed, validation_error = self._validate_output(
                actual_output, test_case.expected_output, test_case.validation_rules
            )
            
            execution_time = time.time() - start_time
            
            # Check performance validation rules
            perf_passed, perf_error = self._validate_performance(
                execution_time, test_case.validation_rules
            )
            
            # Combine validation results
            overall_passed = validation_passed and perf_passed
            if not overall_passed:
                error = validation_error or perf_error
                
        except TestRunnerError:
            # Re-raise TestRunnerError for immediate handling
            raise
        except Exception as e:
            execution_time = time.time() - start_time
            error = f"Execution failed: {str(e)}"
            overall_passed = False
        
        return TestResult(
            test_case_name=test_case.name,
            passed=overall_passed,
            actual_output=actual_output,
            expected_output=test_case.expected_output,
            execution_time=execution_time,
            error=error
        )
    
    def run_test_suite(self) -> List[TestResult]:
        """
        Run all test cases in the configured test suite.
        
        Returns:
            List of TestResult objects for all test cases
            
        Raises:
            TestRunnerError: If no test suite is configured
        """
        if self.test_suite is None:
            raise TestRunnerError("No test suite configured")
            
        self.is_running = True
        results = []
        
        try:
            for test_case in self.test_suite.test_cases:
                result = self.run_test_case(test_case)
                results.append(result)
                
            self.test_results = results
            return results
            
        finally:
            self.is_running = False
    
    def _validate_output(self, actual: str, expected: str, rules: List[str]) -> tuple[bool, Optional[str]]:
        """
        Validate actual output against expected output using specified rules.
        
        Args:
            actual: Actual output from prompt template
            expected: Expected output for comparison
            rules: List of validation rules to apply
            
        Returns:
            Tuple of (validation_passed, error_message)
        """
        for rule in rules:
            # Parse rule and parameters
            rule_parts = rule.split(":")
            rule_name = rule_parts[0]
            rule_params = rule_parts[1:] if len(rule_parts) > 1 else []
            
            if rule_name == ValidationRule.EXACT_MATCH.value:
                if actual != expected:
                    return False, f"exact_match failed: Expected '{expected}', Actual '{actual}'"
                    
            elif rule_name == ValidationRule.CONTAINS.value:
                if expected not in actual:
                    return False, f"contains failed: '{expected}' not found in '{actual}'"
                    
            elif rule_name == ValidationRule.REGEX_MATCH.value:
                pattern = expected if not rule_params else rule_params[0]
                if not re.search(pattern, actual):
                    return False, f"regex_match failed: Pattern '{pattern}' not found in '{actual}'"
                    
            elif rule_name == ValidationRule.LENGTH_RANGE.value:
                if rule_params:
                    min_len, max_len = map(int, rule_params[0].split(","))
                else:
                    min_len, max_len = map(int, expected.split(","))
                    
                actual_len = len(actual)
                if not (min_len <= actual_len <= max_len):
                    return False, f"length_range failed: Length {actual_len} not in range [{min_len}, {max_len}]"
                    
            elif rule_name == ValidationRule.CONTAINS_NAME.value:
                # Check if output contains some form of name
                # This is a basic implementation - can be enhanced
                if not any(word.istitle() for word in actual.split()):
                    return False, f"contains_name failed: No capitalized name found in '{actual}'"
                    
            elif rule_name == ValidationRule.GREETING_FORMAT.value:
                # Check if output follows a greeting format
                greeting_words = ["hello", "hi", "welcome", "greetings"]
                if not any(word.lower() in actual.lower() for word in greeting_words):
                    return False, f"greeting_format failed: No greeting words found in '{actual}'"
                    
            elif rule_name == ValidationRule.HANDLES_EMPTY_INPUT.value:
                # Check if output handles empty input gracefully (no crashes, reasonable output)
                if len(actual.strip()) == 0:
                    return False, f"handles_empty_input failed: Output is empty or whitespace only"
                    
            elif rule_name == ValidationRule.MAX_EXECUTION_TIME.value:
                # Skip performance rules in output validation - handled separately
                continue
            else:
                # Unknown validation rule
                raise TestRunnerError(f"Unknown validation rule: {rule_name}")
        
        return True, None
    
    def _validate_performance(self, execution_time: float, rules: List[str]) -> tuple[bool, Optional[str]]:
        """
        Validate performance metrics against specified rules.
        
        Args:
            execution_time: Time taken to execute the prompt in seconds
            rules: List of validation rules to check
            
        Returns:
            Tuple of (validation_passed, error_message)
        """
        for rule in rules:
            rule_parts = rule.split(":")
            rule_name = rule_parts[0]
            rule_params = rule_parts[1:] if len(rule_parts) > 1 else []
            
            if rule_name == ValidationRule.MAX_EXECUTION_TIME.value:
                max_time = float(rule_params[0]) if rule_params else 1.0
                if execution_time > max_time:
                    return False, f"execution_time exceeded: {execution_time:.3f}s > {max_time}s"
        
        return True, None
    
    def generate_summary_report(self, results: List[TestResult]) -> Dict[str, Any]:
        """
        Generate summary report from test results.
        
        Args:
            results: List of TestResult objects
            
        Returns:
            Dictionary containing summary statistics
        """
        total_tests = len(results)
        passed_tests = sum(1 for result in results if result.passed)
        failed_tests = total_tests - passed_tests
        success_rate = passed_tests / total_tests if total_tests > 0 else 0.0
        
        execution_times = [result.execution_time for result in results]
        execution_stats = {
            "min": min(execution_times) if execution_times else 0.0,
            "max": max(execution_times) if execution_times else 0.0,
            "avg": sum(execution_times) / len(execution_times) if execution_times else 0.0,
            "total": sum(execution_times)
        }
        
        return {
            "test_suite_name": self.test_suite.name if self.test_suite else "Unknown",
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": success_rate,
            "execution_time_stats": execution_stats
        }
    
    def generate_detailed_report(self, results: List[TestResult]) -> Dict[str, Any]:
        """
        Generate detailed report with all test results and metadata.
        
        Args:
            results: List of TestResult objects
            
        Returns:
            Dictionary containing detailed report information
        """
        summary = self.generate_summary_report(results)
        
        test_results = []
        for result in results:
            test_results.append({
                "test_case_name": result.test_case_name,
                "passed": result.passed,
                "actual_output": result.actual_output,
                "expected_output": result.expected_output,
                "execution_time": result.execution_time,
                "error": result.error
            })
        
        prompt_model_info = {
            "name": getattr(self.prompt_model, 'name', 'Unknown'),
            "template": getattr(self.prompt_model, 'template', 'Unknown'),
            "version": getattr(self.prompt_model, 'version', 'Unknown')
        }
        
        return {
            "summary": summary,
            "test_results": test_results,
            "prompt_model_info": prompt_model_info
        }
    
    def export_results_to_json(self, results: List[TestResult]) -> str:
        """
        Export test results to JSON format.
        
        Args:
            results: List of TestResult objects
            
        Returns:
            JSON string representation of the detailed report
        """
        detailed_report = self.generate_detailed_report(results)
        return json.dumps(detailed_report, indent=2, default=str)