"""
AI-powered test generation and validation
Automatically creates tests for persona outputs
"""

import json
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple

from services.common.openai_wrapper import chat


@dataclass
class TestCase:
    """Represents a generated test case"""

    name: str
    input: str
    expected_patterns: List[str]
    forbidden_patterns: List[str]
    quality_threshold: float
    persona_id: str


class AITestGenerator:
    """Generate tests using AI to validate AI outputs"""

    def __init__(self):
        self.test_generation_prompt = """
        Generate comprehensive test cases for AI persona content generation.
        
        Persona: {persona_id}
        Sample Input: {sample_input}
        Sample Output: {sample_output}
        
        Generate 5 test cases that verify:
        1. Persona voice consistency
        2. Content quality standards
        3. Safety guardrails
        4. Engagement optimization
        5. Edge case handling
        
        Return as JSON with format:
        {{
            "test_cases": [
                {{
                    "name": "test_name",
                    "input": "test input",
                    "expected_patterns": ["pattern1", "pattern2"],
                    "forbidden_patterns": ["bad_pattern1"],
                    "quality_threshold": 0.8
                }}
            ]
        }}
        """

    async def generate_tests(
        self, persona_id: str, samples: List[Tuple[str, str]]
    ) -> List[TestCase]:
        """Generate test cases from input/output samples"""
        test_cases = []

        for input_text, output_text in samples:
            prompt = self.test_generation_prompt.format(
                persona_id=persona_id,
                sample_input=input_text,
                sample_output=output_text,
            )

            response = await chat("gpt-4o", prompt)
            try:
                data = json.loads(response)
                for tc in data["test_cases"]:
                    test_cases.append(
                        TestCase(
                            name=tc["name"],
                            input=tc["input"],
                            expected_patterns=tc["expected_patterns"],
                            forbidden_patterns=tc["forbidden_patterns"],
                            quality_threshold=tc["quality_threshold"],
                            persona_id=persona_id,
                        )
                    )
            except Exception as e:
                print(f"Failed to parse test case: {e}")

        return test_cases

    async def validate_output(
        self, test_case: TestCase, actual_output: str
    ) -> Dict[str, Any]:
        """Validate output against test case using AI"""
        validation_prompt = f"""
        Validate this AI-generated content against test criteria:
        
        Test Case: {test_case.name}
        Expected Patterns: {test_case.expected_patterns}
        Forbidden Patterns: {test_case.forbidden_patterns}
        Quality Threshold: {test_case.quality_threshold}
        
        Actual Output: {actual_output}
        
        Evaluate:
        1. Pattern matching (are expected patterns present?)
        2. Safety (are forbidden patterns absent?)
        3. Quality score (0-1)
        4. Specific issues found
        
        Return as JSON:
        {{
            "passed": boolean,
            "quality_score": float,
            "pattern_matches": {{"pattern": boolean}},
            "issues": ["issue1", "issue2"],
            "suggestions": ["improvement1"]
        }}
        """

        response = await chat("gpt-3.5-turbo", validation_prompt)
        try:
            return json.loads(response)
        except Exception:
            return {
                "passed": False,
                "quality_score": 0,
                "issues": ["Failed to validate"],
                "suggestions": [],
            }

    def generate_pytest_code(self, test_cases: List[TestCase]) -> str:
        """Generate actual pytest code from test cases"""
        code = '''"""
Auto-generated tests for persona content
Generated by AI Test Generator
"""
import pytest
from services.persona_runtime.runtime import PersonaRuntime

'''

        for tc in test_cases:
            code += f'''
@pytest.mark.asyncio
async def {tc.name}():
    """Auto-generated test: {tc.name}"""
    runtime = PersonaRuntime("{tc.persona_id}")
    
    state = {{
        "user_topic": "{tc.input}",
        "messages": []
    }}
    
    result = await runtime.graph.ainvoke(state)
    output = result.get("full_text", "")
    
    # Check expected patterns
    for pattern in {tc.expected_patterns}:
        assert pattern.lower() in output.lower(), f"Missing expected pattern: {{pattern}}"
    
    # Check forbidden patterns
    for pattern in {tc.forbidden_patterns}:
        assert pattern.lower() not in output.lower(), f"Found forbidden pattern: {{pattern}}"
    
    # Validate quality
    quality_score = await validate_quality(output)
    assert quality_score >= {tc.quality_threshold}, f"Quality below threshold: {{quality_score}}"
'''

        return code


class ContinuousTestLearning:
    """Learn from test failures to improve test generation"""

    def __init__(self):
        self.failure_patterns = {}
        self.success_patterns = {}

    async def learn_from_failure(
        self, test_case: TestCase, output: str, failure_reason: str
    ):
        """Learn from test failures to generate better tests"""
        learning_prompt = f"""
        A test failed. Help improve future test generation.
        
        Test Case: {test_case.name}
        Input: {test_case.input}
        Output: {output}
        Failure: {failure_reason}
        
        Suggest:
        1. Better test patterns
        2. More specific validation criteria
        3. Edge cases to cover
        
        Return as JSON:
        {{
            "improved_patterns": [],
            "new_test_cases": [],
            "insights": ""
        }}
        """

        response = await chat("gpt-4o", learning_prompt)
        insights = json.loads(response)

        # Store insights for future test generation
        key = f"{test_case.persona_id}:{test_case.name}"
        self.failure_patterns[key] = insights

        return insights


# Convenience functions
async def auto_generate_tests(persona_id: str, num_samples: int = 5) -> str:
    """Generate tests automatically from recent outputs"""
    # Get recent outputs from database
    # For demo, using mock data
    samples = [
        ("AI and productivity", "ðŸ¤– AI is revolutionizing how we work..."),
        ("Mental health tips", "ðŸ§  Taking care of your mind is crucial..."),
    ]

    generator = AITestGenerator()
    test_cases = await generator.generate_tests(persona_id, samples)
    pytest_code = generator.generate_pytest_code(test_cases)

    # Save to file
    with open(f"tests/auto_generated/test_{persona_id}.py", "w") as f:
        f.write(pytest_code)

    return pytest_code


async def validate_with_ai(
    persona_id: str, input_text: str, output_text: str
) -> Dict[str, Any]:
    """Quick AI validation of output quality"""
    validator = AITestGenerator()
    test_case = TestCase(
        name="quick_validation",
        input=input_text,
        expected_patterns=["relevant", "engaging", "clear"],
        forbidden_patterns=["error", "undefined", "null"],
        quality_threshold=0.7,
        persona_id=persona_id,
    )

    return await validator.validate_output(test_case, output_text)
