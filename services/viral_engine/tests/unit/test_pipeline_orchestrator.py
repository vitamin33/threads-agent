# services/viral_engine/tests/unit/test_pipeline_orchestrator.py
from __future__ import annotations

from unittest.mock import AsyncMock, patch

import pytest

from services.viral_engine.pipeline_orchestrator import PipelineOrchestrator


@pytest.fixture
def pipeline_orchestrator():
    """Create PipelineOrchestrator instance for testing with mocked hook optimizer"""
    with patch(
        "services.viral_engine.pipeline_orchestrator.HookOptimizer"
    ) as mock_hook_class:
        # Create a mock instance
        mock_hook_instance = AsyncMock()
        mock_hook_instance.optimize_hook = AsyncMock(
            return_value={
                "optimized_hook": "ðŸš€ Test content",
                "pattern": {"name": "emoji_hook"},
                "predicted_ctr_boost": 0.15,
            }
        )
        mock_hook_class.return_value = mock_hook_instance

        orchestrator = PipelineOrchestrator(
            quality_threshold=0.7,
            enable_hook_optimization=True,
            enable_reply_magnets=True,
        )

        # Set lower threshold for quality gate to make tests pass
        orchestrator.quality_gate.set_threshold(0.05)

        return orchestrator


@pytest.mark.asyncio
class TestPipelineOrchestrator:
    """Test suite for PipelineOrchestrator"""

    async def test_initialization(self, pipeline_orchestrator):
        """Test pipeline orchestrator initializes correctly"""
        assert isinstance(pipeline_orchestrator, PipelineOrchestrator)
        assert pipeline_orchestrator.quality_gate is not None
        assert pipeline_orchestrator.hook_optimizer is not None
        assert pipeline_orchestrator.reply_magnetizer is not None
        assert pipeline_orchestrator.enable_hook_optimization is True
        assert pipeline_orchestrator.enable_reply_magnets is True

    async def test_successful_pipeline_flow(self, pipeline_orchestrator):
        """Test successful flow through entire pipeline"""
        content = "AI will revolutionize healthcare in the next decade"
        persona_id = "ai-elon"

        # Mock the hook optimizer to return a specific result
        if pipeline_orchestrator.hook_optimizer:
            pipeline_orchestrator.hook_optimizer.optimize_hook = AsyncMock(
                return_value={
                    "optimized_hook": f"ðŸš€ {content}",
                    "pattern": {"name": "emoji_hook"},
                    "predicted_ctr_boost": 0.15,
                }
            )

        result = await pipeline_orchestrator.process_content(content, persona_id)

        # Check overall success
        assert result["success"] is True
        assert "final_content" in result
        assert result["persona_id"] == persona_id

        # Check pipeline stages
        assert "pipeline_stages" in result
        stages = result["pipeline_stages"]

        # Hook optimization should be present
        assert "hook_optimization" in stages
        assert stages["hook_optimization"]["success"] is True

        # Quality gate should pass for optimized content
        assert "quality_gate" in stages
        assert stages["quality_gate"]["passed"] is True

        # Reply magnetizer should be present
        assert "reply_magnetizer" in stages
        assert stages["reply_magnetizer"]["success"] is True

    async def test_quality_gate_rejection(self, pipeline_orchestrator):
        """Test pipeline stops when quality gate rejects content"""
        # Set a higher threshold to ensure rejection
        pipeline_orchestrator.quality_gate.set_threshold(0.5)

        poor_content = "boring post"

        result = await pipeline_orchestrator.process_content(poor_content, "test")

        # Should fail
        assert result["success"] is False
        assert "rejection_reason" in result
        assert "improvement_suggestions" in result

        # Should have quality gate stage but no reply magnetizer
        stages = result["pipeline_stages"]
        assert "quality_gate" in stages
        assert stages["quality_gate"]["passed"] is False
        assert "reply_magnetizer" not in stages

    async def test_disabled_stages(self, pipeline_orchestrator):
        """Test pipeline with disabled stages"""
        # Disable hook optimization and reply magnets
        pipeline_orchestrator.update_configuration(
            enable_hook_optimization=False,
            enable_reply_magnets=False,
        )

        content = "Great content that will pass quality gate!"
        result = await pipeline_orchestrator.process_content(content, "test")

        # Should still succeed
        assert result["success"] is True

        # Only quality gate should be present
        stages = result["pipeline_stages"]
        assert "hook_optimization" not in stages
        assert "quality_gate" in stages
        assert "reply_magnetizer" not in stages

    async def test_batch_processing(self, pipeline_orchestrator):
        """Test batch content processing"""
        # Mock hook optimizer for consistent results
        if pipeline_orchestrator.hook_optimizer:
            pipeline_orchestrator.hook_optimizer.optimize_hook = AsyncMock(
                side_effect=lambda persona_id, content: {
                    "optimized_hook": f"ðŸš€ {content}",
                    "pattern": {"name": "emoji_hook"},
                    "predicted_ctr_boost": 0.15,
                }
            )

        content_items = [
            ("High quality content! What's your take?", "persona1", None),
            ("x", "persona2", {"source": "test"}),  # Very short content
            ("Amazing AI breakthrough announced today!", "persona3", None),
        ]

        results = await pipeline_orchestrator.process_batch(content_items)

        assert len(results) == 3
        assert results[0]["success"] is True  # First should pass with mocked hook
        assert (
            results[1]["success"] is False
        )  # Second fails - too short even with emoji
        assert results[2]["success"] is True  # Third should pass

        # Check metadata propagation
        assert results[1]["metadata"]["source"] == "test"

    async def test_pipeline_analytics(self, pipeline_orchestrator):
        """Test analytics generation"""
        # Mock hook optimizer for consistent results
        if pipeline_orchestrator.hook_optimizer:
            pipeline_orchestrator.hook_optimizer.optimize_hook = AsyncMock(
                side_effect=lambda persona_id, content: {
                    "optimized_hook": f"ðŸš€ {content}",
                    "pattern": {"name": "emoji_hook"},
                    "predicted_ctr_boost": 0.15,
                }
            )

        # Process some content to generate stats
        test_contents = [
            ("Great viral content!", "test1"),
            ("Amazing discovery!", "test2"),
            ("Incredible breakthrough!", "test3"),
        ]

        for content, persona in test_contents:
            await pipeline_orchestrator.process_content(content, persona)

        analytics = pipeline_orchestrator.get_pipeline_analytics()

        # Check structure
        assert "pipeline_stats" in analytics
        assert "quality_gate_analytics" in analytics
        assert "magnet_performance" in analytics
        assert "rates" in analytics

        # Check stats
        stats = analytics["pipeline_stats"]
        assert stats["total_processed"] == 3
        assert stats["passed_quality_gate"] == 3  # All pass with mocked hook

        # Check rates
        rates = analytics["rates"]
        assert rates["quality_pass_rate"] == 1.0

    async def test_configuration_update(self, pipeline_orchestrator):
        """Test dynamic configuration updates"""
        # Update quality threshold
        pipeline_orchestrator.update_configuration(quality_threshold=0.5)
        assert pipeline_orchestrator.quality_gate.min_score == 0.5

        # Disable features
        with patch("services.viral_engine.pipeline_orchestrator.HookOptimizer"):
            pipeline_orchestrator.update_configuration(
                enable_hook_optimization=False,
                enable_reply_magnets=False,
            )
            assert pipeline_orchestrator.enable_hook_optimization is False
            assert pipeline_orchestrator.enable_reply_magnets is False

    async def test_error_handling(self, pipeline_orchestrator):
        """Test error handling in pipeline"""
        # Set a high threshold to ensure None content fails
        pipeline_orchestrator.quality_gate.set_threshold(0.5)

        # Create a scenario that might cause an error
        # For now, just test with None content
        result = await pipeline_orchestrator.process_content(
            None,
            "test",  # type: ignore
        )

        # None content should fail quality gate (not error)
        assert result["success"] is False
        assert "rejection_reason" in result

    async def test_stats_reset(self, pipeline_orchestrator):
        """Test statistics reset functionality"""
        # Generate some stats
        await pipeline_orchestrator.process_content("Test content", "test")

        # Reset
        pipeline_orchestrator.reset_stats()

        # Check all stats are zero
        stats = pipeline_orchestrator.pipeline_stats
        assert stats["total_processed"] == 0
        assert stats["passed_quality_gate"] == 0
        assert stats["hook_optimized"] == 0
        assert stats["magnets_added"] == 0

    async def test_metadata_propagation(self, pipeline_orchestrator):
        """Test metadata flows through pipeline"""
        content = "Test content for metadata"
        metadata = {"task_id": "12345", "source": "automated", "priority": "high"}

        result = await pipeline_orchestrator.process_content(content, "test", metadata)

        # Metadata should be preserved
        assert result["metadata"] == metadata

        # Should also appear in quality gate evaluation
        if "quality_gate" in result["pipeline_stages"]:
            evaluation = result["pipeline_stages"]["quality_gate"]["evaluation"]
            assert evaluation["metadata"] == metadata
