"""
vLLM Service - High-performance open-source LLM serving
Replaces OpenAI API with 40% cost savings while maintaining quality
"""

import logging
import os
import time
from contextlib import asynccontextmanager
from typing import Dict, List, Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from prometheus_client import Counter, Histogram, generate_latest
from prometheus_client import CONTENT_TYPE_LATEST
from starlette.responses import Response

from .model_manager import vLLMModelManager
from .cost_tracker import CostTracker
from .quality_evaluator import QualityEvaluator

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Enhanced Prometheus metrics for <50ms performance tracking
REQUEST_COUNT = Counter(
    "vllm_requests_total", "Total vLLM requests", ["model", "status"]
)
REQUEST_DURATION = Histogram(
    "vllm_request_duration_seconds",
    "vLLM request duration with <50ms targeting",
    ["model", "optimization_level"],
    buckets=[
        0.005,
        0.01,
        0.02,
        0.05,
        0.1,
        0.2,
        0.5,
        1.0,
        2.0,
        float("inf"),
    ],  # Detailed <50ms buckets
)
TOKENS_GENERATED = Counter(
    "vllm_tokens_generated_total", "Total tokens generated by vLLM", ["model"]
)
COST_SAVINGS = Counter(
    "vllm_cost_savings_usd", "Cost savings compared to OpenAI", ["model"]
)
LATENCY_TARGET_MET = Counter(
    "vllm_latency_target_met_total", "Requests meeting <50ms target", ["model"]
)
CACHE_HITS = Counter(
    "vllm_cache_hits_total", "Response cache hits for performance", ["model"]
)
APPLE_SILICON_OPTIMIZED = Counter(
    "vllm_apple_silicon_requests_total", "Requests on Apple Silicon", ["model"]
)
CIRCUIT_BREAKER_OPEN = Counter(
    "vllm_circuit_breaker_open_total", "Circuit breaker open events", ["model"]
)

# Global model manager
model_manager = None
cost_tracker = None
quality_evaluator = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    global model_manager, cost_tracker, quality_evaluator

    try:
        logger.info("ðŸš€ Starting vLLM service...")

        # Initialize components
        model_manager = vLLMModelManager()
        cost_tracker = CostTracker()
        quality_evaluator = QualityEvaluator()

        # Load optimized model (Llama-3.1-8B-Instruct for Apple Silicon)
        model_name = os.getenv("VLLM_MODEL", "meta-llama/Llama-3.1-8B-Instruct")
        await model_manager.load_model(model_name)

        logger.info(f"âœ… vLLM service ready with model: {model_name}")
        yield

    except Exception as e:
        logger.error(f"âŒ Failed to start vLLM service: {e}")
        raise
    finally:
        # Cleanup
        if model_manager:
            await model_manager.cleanup()
        logger.info("ðŸ›‘ vLLM service stopped")


# Create FastAPI app
app = FastAPI(
    title="vLLM Service",
    description="High-performance open-source LLM serving with 40% cost savings",
    version="1.0.0",
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request/Response models
class ChatCompletionRequest(BaseModel):
    model: str = "llama-3-8b"
    messages: List[Dict[str, str]]
    max_tokens: Optional[int] = 512
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9
    stream: Optional[bool] = False


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict]
    usage: Dict[str, int]
    cost_info: Optional[Dict] = None


class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    gpu_available: bool
    apple_silicon_optimized: bool
    warmup_completed: bool
    memory_usage: Dict[str, float]
    uptime_seconds: float
    performance_target_met: bool  # <50ms latency target


# Startup time tracking
startup_time = time.time()


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Enhanced health check endpoint for Kubernetes with performance monitoring"""
    global model_manager

    # Initialize model_manager if not present (for testing)
    if model_manager is None:
        from .model_manager import vLLMModelManager

        model_manager = vLLMModelManager()

    # Get performance metrics to check <50ms target
    performance_target_met = False
    if model_manager and model_manager.request_count > 0:
        avg_time = model_manager.total_inference_time / model_manager.request_count
        performance_target_met = avg_time < 0.05  # 50ms

    return HealthResponse(
        status="healthy"
        if model_manager and model_manager.is_ready()
        else "initializing",
        model_loaded=model_manager.is_ready() if model_manager else False,
        gpu_available=model_manager.gpu_available if model_manager else False,
        apple_silicon_optimized=model_manager.is_apple_silicon
        if model_manager
        else False,
        warmup_completed=model_manager.warmup_completed if model_manager else False,
        memory_usage=model_manager.get_memory_usage() if model_manager else {},
        uptime_seconds=time.time() - startup_time,
        performance_target_met=performance_target_met,
    )


@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completions endpoint using vLLM
    Provides 40% cost savings while maintaining quality
    """
    global model_manager, cost_tracker, quality_evaluator

    # Initialize components if not present (for testing)
    if model_manager is None:
        from .model_manager import vLLMModelManager
        from .cost_tracker import CostTracker
        from .quality_evaluator import QualityEvaluator

        model_manager = vLLMModelManager()
        cost_tracker = CostTracker()
        quality_evaluator = QualityEvaluator()

        # Load optimized model for testing
        test_model = os.getenv("VLLM_MODEL", "meta-llama/Llama-3.1-8B-Instruct")
        await model_manager.load_model(test_model)

    if not model_manager or not model_manager.is_ready():
        raise HTTPException(status_code=503, detail="Model not ready")

    start_time = time.time()

    try:
        # Generate response using vLLM
        response = await model_manager.generate(
            messages=request.messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
        )

        # Track enhanced metrics for performance monitoring
        duration = time.time() - start_time

        # Determine optimization level for metrics
        optimization_level = "basic"
        if model_manager.is_apple_silicon:
            optimization_level = "apple_silicon"
        if model_manager.warmup_completed:
            optimization_level += "_warmed"

        REQUEST_DURATION.labels(
            model=request.model, optimization_level=optimization_level
        ).observe(duration)
        REQUEST_COUNT.labels(model=request.model, status="success").inc()
        TOKENS_GENERATED.labels(model=request.model).inc(
            response["usage"]["total_tokens"]
        )

        # Track <50ms latency target
        if duration < 0.05:
            LATENCY_TARGET_MET.labels(model=request.model).inc()

        # Track Apple Silicon usage
        if model_manager.is_apple_silicon:
            APPLE_SILICON_OPTIMIZED.labels(model=request.model).inc()

        # Track cache performance (check if this was a cache hit)
        if response.get("performance", {}).get("cache_hit", False):
            CACHE_HITS.labels(model=request.model).inc()

        # Calculate cost savings
        openai_cost = cost_tracker.calculate_openai_cost(
            response["usage"]["total_tokens"]
        )
        vllm_cost = cost_tracker.calculate_vllm_cost(response["usage"]["total_tokens"])
        savings = openai_cost - vllm_cost
        COST_SAVINGS.labels(model=request.model).inc(savings)

        # Add cost information to response
        response["cost_info"] = {
            "vllm_cost_usd": vllm_cost,
            "openai_cost_usd": openai_cost,
            "savings_usd": savings,
            "savings_percentage": (savings / openai_cost * 100)
            if openai_cost > 0
            else 0,
        }

        logger.info(
            f"Generated {response['usage']['total_tokens']} tokens in {duration:.2f}s, "
            f"saved ${savings:.4f} ({savings / openai_cost * 100:.1f}%)"
        )

        return ChatCompletionResponse(**response)

    except Exception as e:
        REQUEST_COUNT.labels(model=request.model, status="error").inc()

        # Track circuit breaker events
        if model_manager and model_manager.circuit_breaker_open:
            CIRCUIT_BREAKER_OPEN.labels(model=request.model).inc()

        logger.error(f"Generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/models")
async def list_models():
    """List available models"""
    return {
        "object": "list",
        "data": [
            {
                "id": "llama-3-8b",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "meta",
                "cost_per_token": 0.0001,  # 40% less than GPT-3.5
                "description": "Llama 3 8B optimized for cost efficiency",
            }
        ],
    }


@app.get("/cost-comparison")
async def cost_comparison():
    """Get cost comparison statistics"""
    global cost_tracker

    if not cost_tracker:
        raise HTTPException(status_code=503, detail="Cost tracker not ready")

    return await cost_tracker.get_comparison_stats()


@app.get("/quality-metrics")
async def quality_metrics():
    """Get quality evaluation metrics compared to OpenAI"""
    global quality_evaluator

    if not quality_evaluator:
        raise HTTPException(status_code=503, detail="Quality evaluator not ready")

    return await quality_evaluator.get_metrics()


@app.get("/performance")
async def performance_metrics():
    """Get comprehensive performance metrics for monitoring and optimization"""
    global model_manager

    if not model_manager:
        raise HTTPException(status_code=503, detail="Model manager not ready")

    return model_manager.get_performance_metrics()


@app.get("/performance/latency")
async def latency_metrics():
    """Get detailed latency metrics for <50ms performance tracking"""
    global model_manager

    if not model_manager:
        raise HTTPException(status_code=503, detail="Model manager not ready")

    if model_manager.request_count == 0:
        return {
            "message": "No requests processed yet",
            "target_latency_ms": 50,
            "status": "no_data",
        }

    avg_latency = model_manager.total_inference_time / model_manager.request_count
    avg_latency_ms = avg_latency * 1000

    return {
        "target_latency_ms": 50,
        "current_average_ms": round(avg_latency_ms, 2),
        "target_met": avg_latency < 0.05,
        "performance_grade": (
            "excellent"
            if avg_latency < 0.02
            else "good"
            if avg_latency < 0.05
            else "needs_optimization"
        ),
        "total_requests": model_manager.request_count,
        "total_tokens": model_manager.total_tokens_generated,
        "throughput_tokens_per_second": round(
            model_manager.total_tokens_generated
            / max(model_manager.total_inference_time, 0.001),
            2,
        ),
        "optimizations_enabled": {
            "apple_silicon": model_manager.is_apple_silicon,
            "warmup_completed": model_manager.warmup_completed,
            "cache_enabled": len(model_manager.inference_cache) > 0,
            "circuit_breaker": not model_manager.circuit_breaker_open,
        },
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8090)
