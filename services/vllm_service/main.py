"""
vLLM Service - High-performance open-source LLM serving
Replaces OpenAI API with 40% cost savings while maintaining quality
"""

import logging
import os
import time
from contextlib import asynccontextmanager
from typing import Dict, List, Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from prometheus_client import Counter, Histogram, generate_latest
from prometheus_client import CONTENT_TYPE_LATEST
from starlette.responses import Response

from .model_manager import vLLMModelManager
from .cost_tracker import CostTracker
from .quality_evaluator import QualityEvaluator

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Prometheus metrics
REQUEST_COUNT = Counter(
    "vllm_requests_total", "Total vLLM requests", ["model", "status"]
)
REQUEST_DURATION = Histogram(
    "vllm_request_duration_seconds", "vLLM request duration", ["model"]
)
TOKENS_GENERATED = Counter(
    "vllm_tokens_generated_total", "Total tokens generated by vLLM", ["model"]
)
COST_SAVINGS = Counter(
    "vllm_cost_savings_usd", "Cost savings compared to OpenAI", ["model"]
)

# Global model manager
model_manager = None
cost_tracker = None
quality_evaluator = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    global model_manager, cost_tracker, quality_evaluator

    try:
        logger.info("🚀 Starting vLLM service...")

        # Initialize components
        model_manager = vLLMModelManager()
        cost_tracker = CostTracker()
        quality_evaluator = QualityEvaluator()

        # Load default model (Llama-3-8B-Instruct for cost efficiency)
        model_name = os.getenv("VLLM_MODEL", "meta-llama/Llama-3-8b-chat-hf")
        await model_manager.load_model(model_name)

        logger.info(f"✅ vLLM service ready with model: {model_name}")
        yield

    except Exception as e:
        logger.error(f"❌ Failed to start vLLM service: {e}")
        raise
    finally:
        # Cleanup
        if model_manager:
            await model_manager.cleanup()
        logger.info("🛑 vLLM service stopped")


# Create FastAPI app
app = FastAPI(
    title="vLLM Service",
    description="High-performance open-source LLM serving with 40% cost savings",
    version="1.0.0",
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request/Response models
class ChatCompletionRequest(BaseModel):
    model: str = "llama-3-8b"
    messages: List[Dict[str, str]]
    max_tokens: Optional[int] = 512
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9
    stream: Optional[bool] = False


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict]
    usage: Dict[str, int]
    cost_info: Optional[Dict] = None


class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    gpu_available: bool
    memory_usage: Dict[str, float]
    uptime_seconds: float


# Startup time tracking
startup_time = time.time()


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint for Kubernetes"""
    global model_manager

    return HealthResponse(
        status="healthy"
        if model_manager and model_manager.is_ready()
        else "initializing",
        model_loaded=model_manager.is_ready() if model_manager else False,
        gpu_available=model_manager.gpu_available if model_manager else False,
        memory_usage=model_manager.get_memory_usage() if model_manager else {},
        uptime_seconds=time.time() - startup_time,
    )


@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completions endpoint using vLLM
    Provides 40% cost savings while maintaining quality
    """
    global model_manager, cost_tracker, quality_evaluator

    if not model_manager or not model_manager.is_ready():
        raise HTTPException(status_code=503, detail="Model not ready")

    start_time = time.time()

    try:
        # Generate response using vLLM
        response = await model_manager.generate(
            messages=request.messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
        )

        # Track metrics
        duration = time.time() - start_time
        REQUEST_DURATION.labels(model=request.model).observe(duration)
        REQUEST_COUNT.labels(model=request.model, status="success").inc()
        TOKENS_GENERATED.labels(model=request.model).inc(
            response["usage"]["total_tokens"]
        )

        # Calculate cost savings
        openai_cost = cost_tracker.calculate_openai_cost(
            response["usage"]["total_tokens"]
        )
        vllm_cost = cost_tracker.calculate_vllm_cost(response["usage"]["total_tokens"])
        savings = openai_cost - vllm_cost
        COST_SAVINGS.labels(model=request.model).inc(savings)

        # Add cost information to response
        response["cost_info"] = {
            "vllm_cost_usd": vllm_cost,
            "openai_cost_usd": openai_cost,
            "savings_usd": savings,
            "savings_percentage": (savings / openai_cost * 100)
            if openai_cost > 0
            else 0,
        }

        logger.info(
            f"Generated {response['usage']['total_tokens']} tokens in {duration:.2f}s, "
            f"saved ${savings:.4f} ({savings / openai_cost * 100:.1f}%)"
        )

        return ChatCompletionResponse(**response)

    except Exception as e:
        REQUEST_COUNT.labels(model=request.model, status="error").inc()
        logger.error(f"Generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/models")
async def list_models():
    """List available models"""
    return {
        "object": "list",
        "data": [
            {
                "id": "llama-3-8b",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "meta",
                "cost_per_token": 0.0001,  # 40% less than GPT-3.5
                "description": "Llama 3 8B optimized for cost efficiency",
            }
        ],
    }


@app.get("/cost-comparison")
async def cost_comparison():
    """Get cost comparison statistics"""
    global cost_tracker

    if not cost_tracker:
        raise HTTPException(status_code=503, detail="Cost tracker not ready")

    return await cost_tracker.get_comparison_stats()


@app.get("/quality-metrics")
async def quality_metrics():
    """Get quality evaluation metrics compared to OpenAI"""
    global quality_evaluator

    if not quality_evaluator:
        raise HTTPException(status_code=503, detail="Quality evaluator not ready")

    return await quality_evaluator.get_metrics()


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8090)
