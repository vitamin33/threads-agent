# For demo purposes, we'll simulate vLLM without heavy dependencies
# In production, this would include: vllm==0.5.5, transformers==4.44.0, torch==2.4.0
# For interview demo, we focus on architecture and cost comparison

# FastAPI and async
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0

# Monitoring and metrics
prometheus-client==0.19.0
opentelemetry-api==1.22.0
opentelemetry-sdk==1.22.0
opentelemetry-instrumentation-fastapi==0.43b0

# Utilities
httpx==0.25.2
aioredis==2.0.1
python-multipart==0.0.6

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1