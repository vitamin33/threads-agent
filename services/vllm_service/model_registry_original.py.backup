"""
Multi-Model Registry for vLLM Service

Integrates with existing MLflow infrastructure while providing vLLM-specific
model management capabilities. Designed for Apple Silicon M4 Max optimization.

Key Features:
- Multi-model configuration management
- Memory-aware model loading/unloading
- Apple Silicon optimization settings
- Content type routing
- MLflow integration for experiment tracking
"""

import asyncio
import logging
import os
import yaml
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Union

# Import existing MLflow infrastructure
try:
    from services.common.mlflow_model_registry_config import configure_mlflow_with_registry, get_mlflow_client
    from services.achievement_collector.mlops.model_registry import ModelStatus
    MLFLOW_AVAILABLE = True
except ImportError as e:
    logging.warning(f"MLflow integration not available: {e}")
    MLFLOW_AVAILABLE = False
    ModelStatus = None

logger = logging.getLogger(__name__)


class ContentType(Enum):
    """Content types for model routing."""
    TWITTER = "twitter"
    LINKEDIN = "linkedin"
    TECHNICAL_ARTICLES = "technical_articles"
    CODE_DOCUMENTATION = "code_documentation"
    GENERAL = "general"
    SPEED = "speed"
    LIGHTWEIGHT = "lightweight"


class ModelLoadState(Enum):
    """Model loading states."""
    UNLOADED = "unloaded"
    LOADING = "loading"
    LOADED = "loaded"
    UNLOADING = "unloading"
    ERROR = "error"


@dataclass
class MemoryRequirements:
    """Memory requirements for a model."""
    base_gb: float
    optimized_gb: float
    minimum_gb: float
    
    def get_target_memory(self, optimization_level: str = "optimized") -> float:
        """Get target memory based on optimization level."""
        if optimization_level == "maximum_performance":
            return self.base_gb
        elif optimization_level == "balanced":
            return self.optimized_gb
        elif optimization_level == "memory_optimized":
            return self.minimum_gb
        else:
            return self.optimized_gb


@dataclass
class OptimizationConfig:
    """Optimization configuration for Apple Silicon."""
    quantization: str = "fp16"
    backend: str = "metal"
    tensor_parallel: bool = False
    speculative_decoding: bool = False
    kv_cache_dtype: str = "fp8"
    mixed_precision: bool = False
    dynamic_batching: bool = False
    aggressive_caching: bool = False
    model_pruning: bool = False
    
    def to_vllm_kwargs(self) -> Dict[str, Any]:
        """Convert to vLLM initialization kwargs."""
        kwargs = {
            "dtype": self.quantization,
            "device": "mps" if self.backend == "metal" else "cpu",
            "tensor_parallel_size": 1,  # Single GPU for Apple Silicon
            "enable_prefix_caching": True,
            "use_v2_block_manager": True,
        }
        
        if self.kv_cache_dtype:
            kwargs["kv_cache_dtype"] = self.kv_cache_dtype
            
        if self.speculative_decoding:
            kwargs["speculative_model"] = None  # Can be configured per model
            
        return kwargs


@dataclass
class PerformanceTargets:
    """Performance targets for a model."""
    target_latency_ms: int
    max_tokens_per_request: int
    concurrent_requests: int
    tokens_per_second_target: Optional[int] = None


@dataclass
class ModelConfig:
    """Configuration for a single model."""
    name: str
    display_name: str
    memory_requirements: MemoryRequirements
    content_types: List[ContentType]
    priority: int
    optimization: OptimizationConfig
    performance: PerformanceTargets
    model_id: str = field(default="")
    load_state: ModelLoadState = field(default=ModelLoadState.UNLOADED)
    last_used: Optional[datetime] = field(default=None)
    load_time: Optional[float] = field(default=None)
    error_count: int = field(default=0)
    
    def __post_init__(self):
        """Post-initialization processing."""
        if not self.model_id:
            self.model_id = self.name.replace("/", "_").replace("-", "_").lower()
        
        # Convert string content types to enum
        if self.content_types and isinstance(self.content_types[0], str):
            self.content_types = [ContentType(ct) for ct in self.content_types]


class MultiModelRegistry:
    """
    Registry for managing multiple vLLM models with Apple Silicon optimization.
    
    Integrates with existing MLflow infrastructure for experiment tracking
    while providing vLLM-specific model management capabilities.
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialize the multi-model registry."""
        self.config_path = config_path or self._get_default_config_path()
        self.config: Dict[str, Any] = {}
        self.models: Dict[str, ModelConfig] = {}
        self.loaded_models: Dict[str, Any] = {}  # Actual vLLM model instances
        self.memory_usage: Dict[str, float] = {}
        self.total_memory_gb = 36.0  # M4 Max default
        self.memory_threshold = 0.85
        
        # MLflow integration
        self.mlflow_client = None
        self.experiment_id = None
        
        # Load configuration
        self._load_config()
        self._setup_mlflow_integration()
        
    def _get_default_config_path(self) -> str:
        """Get default configuration file path."""
        current_dir = Path(__file__).parent
        return str(current_dir / "config" / "multi_model_config.yaml")
    
    def _load_config(self) -> None:
        """Load configuration from YAML file."""
        try:
            with open(self.config_path, 'r') as f:
                self.config = yaml.safe_load(f)
            
            # Extract basic settings
            self.total_memory_gb = self.config.get('total_memory_gb', 36.0)
            self.memory_threshold = self.config.get('memory_safety_threshold', 0.85)
            
            # Load model configurations
            self._load_model_configs()
            
            logger.info(f"Loaded configuration for {len(self.models)} models")
            
        except Exception as e:
            logger.error(f"Failed to load configuration from {self.config_path}: {e}")
            raise
    
    def _load_model_configs(self) -> None:
        """Load individual model configurations."""
        models_config = self.config.get('models', {})
        
        for model_id, model_data in models_config.items():
            try:
                # Create memory requirements
                mem_req = MemoryRequirements(**model_data['memory_requirements'])
                
                # Create optimization config
                opt_config = OptimizationConfig(**model_data['optimization'])
                
                # Create performance targets
                perf_targets = PerformanceTargets(**model_data['performance'])
                
                # Create model config
                model_config = ModelConfig(
                    name=model_data['name'],
                    display_name=model_data['display_name'],
                    memory_requirements=mem_req,
                    content_types=model_data['content_types'],
                    priority=model_data['priority'],
                    optimization=opt_config,
                    performance=perf_targets,
                    model_id=model_id
                )
                
                self.models[model_id] = model_config
                
            except Exception as e:
                logger.error(f"Failed to load config for model {model_id}: {e}")
    
    def _setup_mlflow_integration(self) -> None:
        """Setup MLflow integration using existing infrastructure."""
        if not MLFLOW_AVAILABLE:
            logger.warning("MLflow not available, skipping integration")
            return
            
        try:
            # Use existing MLflow configuration
            configure_mlflow_with_registry()
            self.mlflow_client = get_mlflow_client()
            
            # Get or create experiment
            experiment_name = self.config.get('mlflow_integration', {}).get('experiment_name', 'vllm_multi_model_deployment')
            
            try:
                experiment = self.mlflow_client.get_experiment_by_name(experiment_name)
                if experiment:
                    self.experiment_id = experiment.experiment_id
                else:
                    self.experiment_id = self.mlflow_client.create_experiment(experiment_name)
            except Exception as e:
                logger.warning(f"Failed to setup MLflow experiment: {e}")
                
            logger.info(f"MLflow integration initialized with experiment: {experiment_name}")
            
        except Exception as e:
            logger.error(f"Failed to setup MLflow integration: {e}")
    
    def get_model_config(self, model_id: str) -> Optional[ModelConfig]:
        """Get configuration for a specific model."""
        return self.models.get(model_id)
    
    def get_models_by_content_type(self, content_type: ContentType) -> List[ModelConfig]:
        """Get models that support a specific content type."""
        return [
            model for model in self.models.values()
            if content_type in model.content_types
        ]
    
    def get_model_for_content_type(self, content_type: ContentType) -> Optional[ModelConfig]:
        """Get the best model for a specific content type based on routing rules."""
        routing_config = self.config.get('content_routing', {})
        content_routing = routing_config.get(content_type.value, {})
        
        # Get primary models for this content type
        primary_models = content_routing.get('primary', [])
        fallback_models = content_routing.get('fallback', [])
        
        # Try primary models first (in order)
        for model_id in primary_models:
            if model_id in self.models and self.models[model_id].load_state == ModelLoadState.LOADED:
                return self.models[model_id]
        
        # Try fallback models
        for model_id in fallback_models:
            if model_id in self.models and self.models[model_id].load_state == ModelLoadState.LOADED:
                return self.models[model_id]
        
        # If no specific routing, return any loaded model that supports the content type
        available_models = [
            model for model in self.models.values()
            if content_type in model.content_types and model.load_state == ModelLoadState.LOADED
        ]
        
        if available_models:
            # Return highest priority (lowest number) model
            return min(available_models, key=lambda m: m.priority)
        
        return None
    
    def get_memory_usage(self) -> Dict[str, float]:
        """Get current memory usage by model."""
        return self.memory_usage.copy()
    
    def get_total_memory_usage(self) -> float:
        """Get total memory usage across all loaded models."""
        return sum(self.memory_usage.values())
    
    def get_available_memory(self) -> float:
        """Get available memory for new models."""
        used_memory = self.get_total_memory_usage()
        max_usable = self.total_memory_gb * self.memory_threshold
        return max(0, max_usable - used_memory)
    
    def can_load_model(self, model_id: str, optimization_level: str = "optimized") -> bool:
        """Check if a model can be loaded given current memory constraints."""
        model_config = self.get_model_config(model_id)
        if not model_config:
            return False
        
        if model_config.load_state == ModelLoadState.LOADED:
            return True  # Already loaded
        
        required_memory = model_config.memory_requirements.get_target_memory(optimization_level)
        available_memory = self.get_available_memory()
        
        return available_memory >= required_memory
    
    def get_models_to_unload_for_memory(self, required_memory: float) -> List[str]:
        """Get models that should be unloaded to free up required memory."""
        if self.get_available_memory() >= required_memory:
            return []
        
        # Get unloading order from config
        unloading_order = self.config.get('resource_management', {}).get('unloading_order', [])
        
        # Get currently loaded models in unloading order
        loaded_models = [
            model_id for model_id in unloading_order
            if model_id in self.models and self.models[model_id].load_state == ModelLoadState.LOADED
        ]
        
        models_to_unload = []
        freed_memory = 0.0
        
        for model_id in loaded_models:
            if freed_memory >= required_memory:
                break
                
            models_to_unload.append(model_id)
            freed_memory += self.memory_usage.get(model_id, 0)
        
        return models_to_unload
    
    def update_model_state(self, model_id: str, state: ModelLoadState, **kwargs) -> None:
        """Update model state and metadata."""
        if model_id not in self.models:
            logger.warning(f"Unknown model ID: {model_id}")
            return
        
        model_config = self.models[model_id]
        model_config.load_state = state
        
        # Update additional metadata
        if 'load_time' in kwargs:
            model_config.load_time = kwargs['load_time']
        
        if 'error_count' in kwargs:
            model_config.error_count = kwargs['error_count']
        
        if state == ModelLoadState.LOADED:
            model_config.last_used = datetime.now()
        
        # Update memory usage
        if 'memory_usage' in kwargs:
            self.memory_usage[model_id] = kwargs['memory_usage']
        elif state == ModelLoadState.UNLOADED:
            self.memory_usage.pop(model_id, None)
    
    def get_loading_priority_order(self) -> List[str]:
        """Get the order in which models should be loaded."""
        loading_order = self.config.get('resource_management', {}).get('loading_order', [])
        
        # Filter to only include configured models
        return [model_id for model_id in loading_order if model_id in self.models]
    
    def get_apple_silicon_optimization(self, preset: str = "balanced") -> Dict[str, Any]:
        """Get Apple Silicon optimization settings for a preset."""
        apple_silicon_config = self.config.get('apple_silicon', {})
        presets = apple_silicon_config.get('presets', {})
        
        return presets.get(preset, presets.get('balanced', {}))
    
    def get_content_routing_config(self, content_type: ContentType) -> Dict[str, Any]:
        """Get routing configuration for a content type."""
        routing_config = self.config.get('content_routing', {})
        return routing_config.get(content_type.value, {})
    
    def log_to_mlflow(self, model_id: str, metrics: Dict[str, float], parameters: Dict[str, Any] = None) -> None:
        """Log metrics and parameters to MLflow."""
        if not self.mlflow_client or not self.experiment_id:
            logger.warning("MLflow not configured, skipping logging")
            return
        
        try:
            import mlflow
            
            with mlflow.start_run(experiment_id=self.experiment_id):
                # Log metrics
                for metric_name, value in metrics.items():
                    mlflow.log_metric(f"{model_id}_{metric_name}", value)
                
                # Log parameters
                if parameters:
                    for param_name, value in parameters.items():
                        mlflow.log_param(f"{model_id}_{param_name}", value)
                
                # Log model metadata
                model_config = self.get_model_config(model_id)
                if model_config:
                    mlflow.log_param(f"{model_id}_model_name", model_config.name)
                    mlflow.log_param(f"{model_id}_priority", model_config.priority)
                    mlflow.log_param(f"{model_id}_content_types", ",".join([ct.value for ct in model_config.content_types]))
                
        except Exception as e:
            logger.error(f"Failed to log to MLflow: {e}")
    
    def get_model_summary(self) -> Dict[str, Any]:
        """Get a summary of all models and their current states."""
        summary = {
            "total_models": len(self.models),
            "loaded_models": len([m for m in self.models.values() if m.load_state == ModelLoadState.LOADED]),
            "total_memory_gb": self.total_memory_gb,
            "used_memory_gb": self.get_total_memory_usage(),
            "available_memory_gb": self.get_available_memory(),
            "memory_utilization_percent": (self.get_total_memory_usage() / self.total_memory_gb) * 100,
            "models": {}
        }
        
        for model_id, model_config in self.models.items():
            summary["models"][model_id] = {
                "name": model_config.name,
                "display_name": model_config.display_name,
                "state": model_config.load_state.value,
                "priority": model_config.priority,
                "memory_usage_gb": self.memory_usage.get(model_id, 0),
                "content_types": [ct.value for ct in model_config.content_types],
                "last_used": model_config.last_used.isoformat() if model_config.last_used else None,
                "load_time": model_config.load_time,
                "error_count": model_config.error_count
            }
        
        return summary
    
    def validate_configuration(self) -> List[str]:
        """Validate the configuration and return any errors."""
        errors = []
        
        # Check total memory requirements
        total_min_memory = sum(
            model.memory_requirements.minimum_gb
            for model in self.models.values()
        )
        
        if total_min_memory > self.total_memory_gb * self.memory_threshold:
            errors.append(f"Total minimum memory requirement ({total_min_memory:.1f}GB) exceeds available memory ({self.total_memory_gb * self.memory_threshold:.1f}GB)")
        
        # Check model priorities are unique
        priorities = [model.priority for model in self.models.values()]
        if len(priorities) != len(set(priorities)):
            errors.append("Model priorities must be unique")
        
        # Check content type routing
        routing_config = self.config.get('content_routing', {})
        for content_type, routing in routing_config.items():
            primary_models = routing.get('primary', [])
            for model_id in primary_models:
                if model_id not in self.models:
                    errors.append(f"Unknown model '{model_id}' in routing for '{content_type}'")
        
        return errors


# Global registry instance
_registry_instance: Optional[MultiModelRegistry] = None


def get_model_registry(config_path: Optional[str] = None) -> MultiModelRegistry:
    """Get the global model registry instance."""
    global _registry_instance
    
    if _registry_instance is None:
        _registry_instance = MultiModelRegistry(config_path)
    
    return _registry_instance


def reset_model_registry() -> None:
    """Reset the global model registry instance (for testing)."""
    global _registry_instance
    _registry_instance = None


# Convenience functions
def get_model_config(model_id: str) -> Optional[ModelConfig]:
    """Get configuration for a specific model."""
    return get_model_registry().get_model_config(model_id)


def get_model_for_content_type(content_type: Union[ContentType, str]) -> Optional[ModelConfig]:
    """Get the best model for a specific content type."""
    if isinstance(content_type, str):
        content_type = ContentType(content_type)
    return get_model_registry().get_model_for_content_type(content_type)


def get_models_by_priority() -> List[ModelConfig]:
    """Get all models sorted by priority."""
    registry = get_model_registry()
    return sorted(registry.models.values(), key=lambda m: m.priority)