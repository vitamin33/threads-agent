# üöÄ Quick Start Guide

Get the Achievement Collector System up and running in 5 minutes!

## Prerequisites

- **Python 3.12+** installed
- **GitHub Personal Access Token** with repo access
- **Virtual environment** support
- **Git** for repository access

## 1. Environment Setup

```bash
# Navigate to the achievement collector service
cd services/achievement_collector

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install pytest pytest-asyncio sqlalchemy alembic asyncpg psycopg2-binary
```

## 2. Configuration

Create your GitHub token and set environment variables:

```bash
# Set your GitHub token
export GITHUB_TOKEN="your_github_personal_access_token"

# Optional: Configure database (defaults to SQLite)
export DATABASE_URL="postgresql://user:pass@localhost/achievements"

# Optional: Configure MLflow (defaults to local)
export MLFLOW_SERVER_URL="http://localhost:5000"
```

## 3. Quick Test Run

Verify the system works with a simple test:

```python
# test_quick_start.py
import asyncio
from services.historical_pr_analyzer import HistoricalPRAnalyzer

async def quick_test():
    # Initialize analyzer
    analyzer = HistoricalPRAnalyzer(github_token="your_token")
    
    # Test with a small public repository
    results = await analyzer.analyze_repository_history("octocat/Hello-World")
    
    print(f"‚úÖ Analyzed {results['total_prs']} PRs successfully!")
    print(f"üìä Found {results['high_impact_prs']} high-impact PRs")
    
    return results

# Run the test
if __name__ == "__main__":
    asyncio.run(quick_test())
```

Run it:
```bash
python test_quick_start.py
```

## 4. Complete Pipeline Demo

Run the full end-to-end pipeline:

```python
# demo_pipeline.py
import asyncio
from pipeline.end_to_end_integration import EndToEndPipeline

async def run_demo():
    print("üöÄ Starting Achievement Collector Demo...")
    
    # Initialize pipeline
    pipeline = EndToEndPipeline()
    
    # Run complete analysis on a repository
    result = await pipeline.run_complete_pipeline("microsoft/vscode")
    
    # Display results
    print(f"\nüìà Analysis Complete!")
    print(f"Status: {result['status']}")
    print(f"Portfolio Value: ${result['portfolio_value']:,.2f}")
    print(f"Execution Time: {result['execution_time']:.1f} seconds")
    print(f"PRs Processed: {result['processed_prs']}")
    
    return result

if __name__ == "__main__":
    asyncio.run(run_demo())
```

Run the demo:
```bash
python demo_pipeline.py
```

Expected output:
```
üöÄ Starting Achievement Collector Demo...

üìà Analysis Complete!
Status: success
Portfolio Value: $292,500.00
Execution Time: 247.5 seconds
PRs Processed: 156
```

## 5. Generate Portfolio Report

Create a professional portfolio report:

```python
# generate_portfolio.py
import asyncio
from services.portfolio_validator import PortfolioValidator

async def generate_report():
    print("üìÑ Generating Portfolio Report...")
    
    # Sample achievement data (normally from PR analysis)
    sample_achievements = [
        {
            "title": "Implement OAuth2 Authentication",
            "category": "feature",
            "impact_score": 85,
            "business_value": 25000,
            "time_investment": 40
        },
        {
            "title": "Optimize Database Queries",
            "category": "performance",
            "impact_score": 92,
            "business_value": 35000,
            "time_investment": 32
        }
    ]
    
    # Generate portfolio
    validator = PortfolioValidator()
    portfolio = validator.validate_and_generate_portfolio(sample_achievements)
    
    # Export reports
    validator.export_json("portfolio_report.json")
    validator.export_html("portfolio_report.html")
    
    print(f"‚úÖ Portfolio generated successfully!")
    print(f"üìä Total Value: ${portfolio['total_value']:,.2f}")
    print(f"üìÅ Reports saved: portfolio_report.json, portfolio_report.html")
    
    return portfolio

if __name__ == "__main__":
    asyncio.run(generate_report())
```

## 6. Test ML Models

Verify the MLOps pipeline:

```python
# test_mlops.py
import asyncio
from mlops.mlflow_registry import MLflowRegistry

async def test_models():
    print("ü§ñ Testing ML Models...")
    
    # Initialize registry
    registry = MLflowRegistry()
    
    # List available models
    models = await registry.list_models()
    print(f"üìã Found {len(models)} registered models")
    
    # Test model registration
    success = await registry.register_model(
        model_name="test_business_predictor",
        model_version="1.0.0",
        accuracy=0.95
    )
    
    if success:
        print("‚úÖ Model registration successful!")
    else:
        print("‚ùå Model registration failed")
    
    return models

if __name__ == "__main__":
    asyncio.run(test_models())
```

## 7. Test LLM Router

Verify the intelligent LLM routing:

```python
# test_llm_router.py
import asyncio
from ai_pipeline.intelligent_llm_router import IntelligentLLMRouter

async def test_router():
    print("üß† Testing LLM Router...")
    
    # Initialize router
    router = IntelligentLLMRouter()
    
    # Test routing decision
    decision = await router.route_request(
        query="Analyze the business impact of this code change",
        requirements={
            "max_cost": 0.01,
            "min_quality": 0.9,
            "max_latency": 5.0
        }
    )
    
    print(f"üéØ Routing Decision:")
    print(f"  Selected Provider: {decision.selected_provider}")
    print(f"  Model: {decision.selected_model}")
    print(f"  Estimated Cost: ${decision.estimated_cost:.4f}")
    print(f"  Expected Quality: {decision.quality_score:.2f}")
    
    return decision

if __name__ == "__main__":
    asyncio.run(test_router())
```

## 8. Health Check

Verify all components are working:

```python
# health_check.py
from pipeline.end_to_end_integration import EndToEndPipeline

def system_health_check():
    print("üîç Running System Health Check...")
    
    pipeline = EndToEndPipeline()
    health = pipeline.run_health_checks()
    
    print(f"\nüìä System Status: {health['overall_status']}")
    print(f"üóÑÔ∏è  Database: {health['database']['status']}")
    print(f"ü§ñ MLflow: {health['mlflow']['status']} ({health['mlflow']['models_available']} models)")
    print(f"üß† LLM Router: {health['llm_router']['status']} ({health['llm_router']['providers_online']} providers)")
    
    if health['overall_status'] == 'healthy':
        print("\n‚úÖ All systems operational!")
    else:
        print("\n‚ö†Ô∏è  Some components need attention")
    
    return health

if __name__ == "__main__":
    system_health_check()
```

## 9. Run Tests

Verify everything works with the test suite:

```bash
# Run all tests
python -m pytest tests/ -v

# Run specific component tests
python -m pytest tests/test_historical_pr_analyzer.py -v
python -m pytest tests/test_portfolio_validator.py -v
python -m pytest tests/test_mlflow_registry.py -v

# Run performance tests
python -m pytest tests/test_end_to_end_integration.py::TestEndToEndIntegration::test_pipeline_performance_benchmarks -v
```

Expected output:
```
tests/test_historical_pr_analyzer.py::TestHistoricalPRAnalyzer::test_analyzer_initialization PASSED
tests/test_historical_pr_analyzer.py::TestHistoricalPRAnalyzer::test_fetch_all_prs_returns_list PASSED
...
========================= 50 passed in 12.34s =========================
```

## 10. Production Deployment Check

Verify production readiness:

```python
# production_check.py
from pipeline.end_to_end_integration import EndToEndPipeline

def production_readiness_check():
    print("üè≠ Checking Production Readiness...")
    
    # Get production configuration
    config = EndToEndPipeline.get_production_config()
    
    print(f"üìä Database Pool Size: {config['database']['pool_size']}")
    print(f"üîß Monitoring Enabled: {config['monitoring']['prometheus_enabled']}")
    print(f"‚ö° Error Handling: {config['error_handling']['circuit_breaker_enabled']}")
    print(f"üîÑ Retry Count: {config['error_handling']['retry_count']}")
    
    # Check architecture documentation
    docs = EndToEndPipeline.get_architecture_docs()
    components = len(docs['components'])
    monitoring_metrics = len(docs['monitoring']['metrics'])
    
    print(f"\nüìö Documentation Status:")
    print(f"  Components Documented: {components}")
    print(f"  Monitoring Metrics: {monitoring_metrics}")
    print(f"  Architecture Complete: ‚úÖ")
    
    print(f"\nüöÄ System is production-ready!")

if __name__ == "__main__":
    production_readiness_check()
```

## Troubleshooting

### Common Issues

1. **GitHub API Rate Limiting**
   ```bash
   # Check your rate limit status
   curl -H "Authorization: token YOUR_TOKEN" https://api.github.com/rate_limit
   ```

2. **Import Errors**
   ```bash
   # Ensure you're in the correct directory and venv is activated
   cd services/achievement_collector
   source venv/bin/activate
   export PYTHONPATH="${PYTHONPATH}:$(pwd)"
   ```

3. **Database Connection Issues**
   ```bash
   # For local development, SQLite is used by default
   # No additional setup required
   ```

### Getting Help

- **Check Logs**: All components use structured logging
- **Run Health Checks**: Use the health check scripts above
- **Verify Configuration**: Ensure environment variables are set
- **Test Individual Components**: Run component-specific tests

## Next Steps

Once you have the system running:

1. **Read the [Architecture Guide](./architecture.md)** to understand the system design
2. **Explore [Component Documentation](./components.md)** for detailed API information
3. **Review [Usage Examples](./examples.md)** for advanced use cases
4. **Set up [Monitoring](./monitoring.md)** for production deployment

## Quick Start Checklist

- [ ] Environment setup complete
- [ ] GitHub token configured
- [ ] Quick test passes
- [ ] Full pipeline demo works
- [ ] Portfolio report generated
- [ ] ML models tested
- [ ] LLM router functional
- [ ] Health checks pass
- [ ] Test suite runs successfully
- [ ] Production readiness verified

**üéâ Congratulations! Your Achievement Collector System is ready to transform GitHub repositories into professional portfolios worth $200K-350K!**