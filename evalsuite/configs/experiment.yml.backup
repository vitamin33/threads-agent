# Apple Silicon Local LM Evaluation Configuration
dataset_dir: evalsuite/data/prompts
output_dir: outputs
stacks: ["mlx", "llamacpp", "mps"]

models:
  # Modern Models (Priority Targets)
  - id: qwen2.5-7b-instruct
    hf_model: Qwen/Qwen2.5-7B-Instruct
    stack: mlx
    license: apache-2.0
    gated: false
    priority: modern
    notes: "Qwen 2.5 instruction-tuned, 7B params"
    
  - id: mistral-7b-instruct-v0.3
    hf_model: mistralai/Mistral-7B-Instruct-v0.3
    stack: mlx
    license: apache-2.0
    gated: false
    priority: modern
    notes: "Mistral 7B Instruct v0.3, strong performance"
    
  - id: phi-3.5-instruct
    hf_model: microsoft/Phi-3.5-mini-instruct
    stack: llamacpp
    license: mit
    gated: false
    priority: modern
    notes: "Phi-3.5 Mini Instruct, efficient architecture"
    
  - id: olmo-7b-instruct
    hf_model: allenai/OLMo-7B-Instruct
    stack: mlx
    license: apache-2.0
    gated: false
    priority: modern
    notes: "OLMo 7B Instruct, fully open model"
    
  - id: yi-1.5-6b-chat
    hf_model: 01-ai/Yi-1.5-6B-Chat
    stack: llamacpp
    license: apache-2.0
    gated: false
    priority: modern
    notes: "Yi-1.5 6B Chat, competitive performance"
  
  # Legacy Baseline
  - id: opt-2.7b
    hf_model: facebook/opt-2.7b
    stack: mps
    license: mit
    gated: false
    priority: legacy
    notes: "Legacy baseline for comparison"

sampling:
  temperature: 0.2
  top_p: 0.9
  max_new_tokens: [128, 384]

seeds: [17, 23, 42]

power:
  kwh_rate: 0.15
  m4_max_tdp: 35

cloud_baseline:
  name: small-instruct
  price_per_1k_output_tokens: 0.15

mlflow:
  tracking_uri: file:./mlruns
  experiment: apple_silicon_local_lm_v2

performance:
  warmup_runs: 5
  timed_runs: 20
  memory_monitoring: true
  
judge:
  provider: openai
  model: gpt-4o-mini
  max_retries: 3
  timeout_seconds: 30