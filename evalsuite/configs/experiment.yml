# Apple Silicon Local LM Evaluation Configuration
dataset_dir: evalsuite/data/prompts
output_dir: outputs
stacks: ["mlx", "llamacpp", "mps"]

models:
  # Locally Available Models (Verified in cache)
  - id: opt-2.7b
    hf_model: facebook/opt-2.7b
    stack: mps
    license: mit
    gated: false
    notes: "Previous champion: 8.40±0.78/10"
    
  - id: bloom-560m
    hf_model: bigscience/bloom-560m
    stack: mps
    license: bigscience-bloom-rail-1.0
    gated: false
    notes: "Previous runner-up: 7.70±0.59/10"
    
  - id: gpt-neo-1.3b
    hf_model: EleutherAI/gpt-neo-1.3B
    stack: mps
    license: mit
    gated: false
    notes: "GPT-Neo architecture, mid-size"
    
  - id: tinyllama-1.1b-chat
    hf_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
    stack: mps
    license: apache-2.0
    gated: false
    notes: "Optimized chat model, efficient"
    
  - id: bloom-3b
    hf_model: bigscience/bloom-3b
    stack: mps
    license: bigscience-bloom-rail-1.0
    gated: false
    notes: "Larger BLOOM variant"
    
  # - id: nemotron-nano-9b
  #   hf_model: nvidia/NVIDIA-Nemotron-Nano-9B-v2
  #   stack: mps
  #   license: nvidia-open-model-license
  #   gated: false
  #   notes: "Enterprise-grade, requires mamba-ssm package"

sampling:
  temperature: 0.2
  top_p: 0.9
  max_new_tokens: [128, 384]

seeds: [17, 23, 42]

power:
  kwh_rate: 0.15
  m4_max_tdp: 35

cloud_baseline:
  name: small-instruct
  price_per_1k_output_tokens: 0.15

mlflow:
  tracking_uri: file:./mlruns
  experiment: apple_silicon_local_lm_v2

performance:
  warmup_runs: 5
  timed_runs: 20
  memory_monitoring: true
  
judge:
  provider: openai
  model: gpt-4o-mini
  max_retries: 3
  timeout_seconds: 30