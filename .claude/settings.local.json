{
  "permissions": {
    "allow": [
      "mcp__linear__linear_searchIssues",
      "mcp__linear__linear_getIssues",
      "Bash(find:*)",
      "mcp__linear__linear_getViewer",
      "mcp__linear__linear_getTeams",
      "Bash(kubectl get:*)",
      "Bash(helm list:*)",
      "mcp__filesystem__read_multiple_files",
      "mcp__filesystem__read_file",
      "mcp__linear__linear_getIssueById",
      "Bash(mkdir:*)",
      "Bash(python -m pytest services/threads_adaptor/tests/unit/test_limiter.py -v)",
      "Bash(python3 -m pytest services/threads_adaptor/tests/unit/test_limiter.py -v)",
      "Bash(ls:*)",
      "Bash(python -m pytest services/threads_adaptor/tests/unit/test_limiter.py::TestTokenBucketLimiter::test_acquire_nowait_failure -xvs)",
      "Bash(python -m pytest services/threads_adaptor/tests/unit/test_limiter.py::TestHighVolumeRateLimiting::test_25_calls_inserts_pauses -xvs)",
      "Bash(python3:*)",
      "Bash(timeout 10s python -m pytest:*)",
      "Bash(python -m pytest services/threads_adaptor/tests/unit/test_limiter.py::TestHighVolumeRateLimiting::test_25_calls_inserts_pauses -xvs --timeout=10)",
      "Bash(python -m pytest services/threads_adaptor/tests/unit/test_limiter.py::TestHighVolumeRateLimiting::test_burst_then_steady_rate -xvs)",
      "Bash(mypy:*)",
      "Bash(just ship:*)",
      "Bash(python -m pytest services/threads_adaptor/tests/unit/test_limiter.py::TestTokenBucketLimiter::test_token_refill -xvs)",
      "mcp__filesystem__search_files",
      "Bash(curl:*)",
      "Bash(pkill:*)",
      "Bash(true)",
      "Bash(kubectl logs:*)",
      "Bash(chmod:*)",
      "Bash(./scripts/quality-gates.sh:*)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH pytest -s -m e2e tests/e2e/test_post_flow.py::test_draft_post_happy_path)",
      "Bash(just lint)",
      "Bash(pip install:*)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH pytest -s -m e2e)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH pytest -q)",
      "Bash(source:*)",
      "Bash(isort:*)",
      "Bash(just pre-commit-check:*)",
      "Bash(ruff format:*)",
      "Bash(just pre-commit-fix:*)",
      "Bash(./scripts/learning-system.sh:*)",
      "Bash(just analyze-patterns:*)",
      "Bash(just benchmark-performance:*)",
      "Bash(just learning-report:*)",
      "Bash(just learning-dashboard:*)",
      "Bash(./scripts/workflow-automation.sh:*)",
      "Bash(just workflow-dashboard:*)",
      "Bash(just orchestrate:*)",
      "Bash(just:*)",
      "Bash(just check)",
      "Bash(git checkout:*)",
      "Bash(git pull:*)",
      "Bash(OPENAI_MOCK=1 PYTHONPATH=$PWD:$PYTHONPATH pytest -k \"test_draft_post_happy_path\" -s)",
      "Bash(OPENAI_MOCK=1 PYTHONPATH=$PWD:$PYTHONPATH python -c \"\nimport os\nos.environ[''OPENAI_MOCK''] = ''1''\nfrom services.common.openai_wrapper import chat, _usd\nresult = chat(''gpt-4o'', ''test prompt'')\ncost = _usd(''gpt-4o'', 10, 10)\nprint(f''Result: {result}'')\nprint(f''Cost: {cost}'')\nprint(''OPENAI_MOCK mode working correctly!'')\n\")",
      "Bash(OPENAI_MOCK=1 just e2e)",
      "mcp__linear__linear_getProjects",
      "mcp__linear__linear_createProject",
      "mcp__linear__linear_createIssue",
      "mcp__linear__linear_updateIssue",
      "Bash(jq:*)",
      "Bash(cat:*)",
      "Bash(./scripts/command-center.sh generate:*)",
      "Bash(rm:*)",
      "Bash(./debug-cc.sh:*)",
      "Bash(quality_data='{\"\"\"\"quality_score\"\"\"\": 88}')",
      "Bash(echo \"$quality_data\")",
      "Bash(./scripts/command-center-simple.sh generate:*)",
      "Bash(./scripts/cc-working.sh generate:*)",
      "Bash(./scripts/cc-final.sh:*)",
      "Bash(./test-cc.sh:*)",
      "Bash(OPENAI_MOCK=1 PYTHONPATH=$PWD:$PYTHONPATH pytest -s -m e2e tests/e2e/test_enhanced_metrics.py::test_business_metrics_collection -v)",
      "mcp__filesystem__list_directory",
      "mcp__filesystem__directory_tree",
      "Bash(just images)",
      "Bash(just deploy-dev:*)",
      "Bash(OPENAI_MOCK=1 PYTHONPATH=$PWD:$PYTHONPATH pytest -s -m e2e tests/e2e/test_enhanced_metrics.py::test_http_request_metrics -v)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(python services/common/tests/test_enhanced_metrics.py)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH python -m pytest services/common/tests/test_enhanced_metrics.py -v)",
      "Bash(just lint)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH python -m pytest services/common/tests/test_enhanced_metrics.py -q)",
      "mcp__linear__linear_getOrganization",
      "Bash(chmod:*)",
      "mcp__linear__linear_createIssue",
      "Bash(npm install:*)",
      "Bash(npm search:*)",
      "Bash(cp:*)",
      "Bash(k3d:*)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH:- pytest -q -m \"not e2e\")",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH:- pytest tests/e2e/test_enhanced_metrics.py::test_business_metrics_collection -v)",
      "Bash(helm upgrade:*)",
      "Bash(kubectl rollout:*)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH:- pytest tests/e2e/test_enhanced_metrics.py::test_business_metrics_collection -v -s)",
      "Bash(docker build:*)",
      "Bash(kubectl describe:*)",
      "Bash(kubectl create secret:*)",
      "Bash(kubectl delete:*)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH:- pytest tests/e2e/test_enhanced_metrics.py -v)",
      "Bash(PYTHONPATH=$PWD:$PYTHONPATH:- pytest tests/e2e/test_post_flow.py::test_draft_post_happy_path -v)",
      "Bash(pytest:*)",
      "Bash(PYTHONPATH=. pytest -v)",
      "Bash(PYTHONPATH=. pytest tests/e2e/test_metrics.py::test_metrics_endpoint -v)",
      "Bash(PYTHONPATH=. pytest -m \"not e2e\" -v)",
      "Bash(PYTHONPATH=. pytest --tb=short)",
      "Bash(PYTHONPATH=. pytest -v --tb=no)",
      "Bash(PYTHONPATH=. pytest -m \"not e2e\" -q)",
      "Bash(PYTHONPATH=. pytest tests/e2e/test_metrics.py -v --tb=short)",
      "Bash(PYTHONPATH=. pytest tests/e2e/test_enhanced_metrics.py::test_business_metrics_collection -v --tb=short)",
      "Bash(kubectl exec:*)",
      "Bash(grep:*)",
      "mcp__linear__linear_getInitiatives",
      "mcp__linear__linear_getProjectIssues",
      "Bash(PYTHONPATH=. pytest services/common/tests/test_enhanced_metrics.py -v)",
      "Bash(PYTHONPATH=. pytest services/common/tests/test_enhanced_metrics.py::test_business_metrics -v)",
      "Bash(git push:*)",
      "Bash(helm template:*)",
      "Bash(kubectl wait:*)",
      "Bash(PYTHONPATH=. pytest -m e2e tests/e2e/test_metrics.py::test_metrics_endpoint -v)",
      "Bash(NO_PR=true just ship \"fix: Improve e2e test reliability with shared port forwarding\n\n- Add shared conftest.py fixture for session-scoped port forwarding\n- Automatically establish port forwards for all e2e tests\n- Add service readiness checks with timeout handling  \n- Clean up port forward processes on test completion\n- Update all e2e test files to use shared configuration\n- Add enhanced e2e-prepare command with service wait conditions\n- Remove duplicate port forwarding code from individual tests\n\nFixes intermittent e2e test failures due to port forwarding issues.\nTests now run reliably across multiple sessions without manual setup.\")",
      "Bash(PYTHONPATH=. python -m pytest services/viral_engine/tests/test_hook_optimizer.py -v)",
      "Bash(helm delete:*)",
      "Bash(git stash:*)",
      "Bash(PYTHONPATH=. pytest -m e2e tests/e2e/test_post_flow.py::test_draft_post_happy_path -v)",
      "Bash(pip show:*)",
      "Bash(PYTHONPATH=. python -m pytest services/viral_engine/tests/unit/test_engagement_predictor.py -v)",
      "Bash(PYTHONPATH=. python -m pytest services/viral_engine/tests/unit/test_engagement_predictor.py::TestEngagementPredictor::test_readability_calculation -v)",
      "Bash(PYTHONPATH=. python -m pytest services/viral_engine/tests/unit/test_engagement_predictor.py::TestEngagementPredictor::test_content_scoring -v)",
      "Bash(PYTHONPATH=/Users/vitaliiserbyn/development/threads-agent python -c \"\nfrom engagement_predictor import EngagementPredictor\npredictor = EngagementPredictor()\nresult = predictor.predict_engagement_rate(''Unpopular opinion: 90% of productivity apps make you LESS productive. Here\\''s why:'')\nprint(f''Prediction: {result[\"\"predicted_engagement_rate\"\"]:.2%}'')\nprint(f''Quality Score: {result[\"\"quality_score\"\"]:.3f}'')\nprint(f''Should Publish: {result[\"\"should_publish\"\"]}'')\n\")",
      "Bash(PYTHONPATH=/Users/vitaliiserbyn/development/threads-agent python -c \"\nfrom engagement_predictor import EngagementPredictor\npredictor = EngagementPredictor()\n\n# Test with a high-quality viral post\nviral_post = ''Stop scrolling! 90% of people fail because of this one mistake. Here\\''s the shocking truth nobody talks about: They chase perfection instead of progress. What\\''s your biggest failure that taught you the most?''\n\nresult = predictor.predict_engagement_rate(viral_post)\nprint(f''High-Quality Post:'')\nprint(f''Prediction: {result[\"\"predicted_engagement_rate\"\"]:.2%}'')\nprint(f''Quality Score: {result[\"\"quality_score\"\"]:.3f}'')\nprint(f''Should Publish: {result[\"\"should_publish\"\"]}'')\nprint(f''Top Factors: {result[\"\"top_factors\"\"][:3]}'')\n\")",
      "Bash(PYTHONPATH=. python:*)",
      "Bash(NO_PR=true just ship \"feat: CRA-228 MVP Engagement Predictor with rule-based scoring\n\n- Implement EngagementPredictor with intelligent rule-based scoring\n- Add AdvancedFeatureExtractor for 20+ text analysis features\n- Create 8 core engagement metrics: readability, emotion, hooks, etc\n- Add FastAPI endpoints: /predict/engagement, /score/content, /score/batch\n- Implement quality gate with 0.6 threshold for content filtering\n- Add comprehensive unit tests (16 tests, 100% pass rate)\n- Include ML training infrastructure for future enhancement\n- Remove sklearn imports from runtime to avoid dependency issues\n\nMVP implementation provides immediate value with rule-based scoring\nthat identifies viral content patterns. Ready for production deployment\nwith future ML enhancement path when training data is available.\n\nTested: All unit tests passing, <200ms latency, quality gate working\")",
      "Bash(python:*)",
      "Bash(black:*)"
    ],
    "deny": []
  }
}