# .github/workflows/dev-ci.yml
name: dev-ci

on:
  pull_request:
    branches: [main]
#   #dev branches keep feedback loop tight
#   push:
#     branches: [cra-*]

jobs:
  test:
    name: py${{ matrix.python }} • k3d
    runs-on: ubuntu-22.04
    timeout-minutes: 15 # fail-fast guard
    strategy:
      fail-fast: false # let both versions run
      matrix:
        python: ["3.12"]

    steps:
      # ————————————————————————————————
      # 0.  Checkout code
      # ————————————————————————————————
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # for proper cache keys

      # ————————————————————————————————
      # 1.  BuildKit + cache
      # ————————————————————————————————
      - name: Set up BuildX
        id: buildx #  ← we’ll need the builder name later
        uses: docker/setup-buildx-action@v3
        with:
          driver: docker-container
          buildkitd-flags: --debug

      - name: Prepare buildx cache
        id: buildx-cache
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      # ————————————————————————————————
      # 2.  Spin local k3d cluster ⬅️ must be *before* image import
      # ————————————————————————————————
      - name: Create k3d cluster
        uses: AbsaOSS/k3d-action@v2
        with:
          cluster-name: dev # => kubectl context "k3d-dev"
          # args: --wait                    # uncomment if you want hard blocking

      # ————————————————————————————————
      # 3.  Build & load images (cache-aware)
      # ————————————————————————————————
      - name: Build & load images
        run: |
          CACHE_OPTS="--cache-from=type=local,src=/tmp/.buildx-cache \
                      --cache-to=type=local,dest=/tmp/.buildx-cache-new,mode=max"

          BUILDER="${{ steps.buildx.outputs.name }}"   # <— builder set above

          # Build services that exist
          BUILT_IMAGES=""
          for svc in orchestrator celery_worker persona_runtime fake_threads viral_engine revenue; do
            if [ -d "services/${svc}" ]; then
              echo "Building $svc..."
              docker buildx build \
                --builder "$BUILDER" \
                --load \
                $CACHE_OPTS \
                -f services/${svc}/Dockerfile \
                -t ${svc//_/-}:local .
              BUILT_IMAGES="$BUILT_IMAGES ${svc//_/-}:local"
            else
              echo "⚠️ Skipping $svc - directory not found"
            fi
          done

          docker pull bitnami/postgresql:16
          docker pull rabbitmq:3.13-management-alpine
          docker pull qdrant/qdrant:v1.9.4

          # Import only built images
          k3d image import \
            $BUILT_IMAGES \
            qdrant/qdrant:v1.9.4 \
            -c dev

      - name: Save buildx cache
        if: steps.buildx-cache.outputs.cache-hit != 'true'
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache

      # ————————————————————————————————
      # 4.  Helm install with CI values
      # ————————————————————————————————
      - name: Helm upgrade (dev cluster)
        id: helm
        continue-on-error: true
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_MOCK: "1"
        run: |
          helm upgrade --install threads ./chart \
            -f chart/values-ci.yaml \
            --set openai.apiKey="$OPENAI_API_KEY" \
            --wait --timeout 300s --debug

      # ─── If Helm failed, dump cluster state ──────────────────────────────
      - name: Dump pod list + events if Helm failed
        if: steps.helm.outcome == 'failure'
        run: |
          echo "❌ Helm release did not become ready. Dumping diagnostics …"
          kubectl get pods -A -o wide
          kubectl get events --sort-by='.lastTimestamp' -A | tail -n 40
          # Show logs of pods stuck in Pending / CrashLoop
          for p in $(kubectl get pods --no-headers | awk '$3!="Running"{print $1}'); do
            echo "──── logs: $p ────"
            kubectl logs "$p" --tail=100 || true
          done
          # Fail the step explicitly so the job still turns red
          exit 1
      - name: Wait for core pods
        run: |
          # deployments
          kubectl rollout status deploy/orchestrator     --timeout=120s
          kubectl rollout status deploy/celery-worker    --timeout=120s
          kubectl rollout status deploy/fake-threads     --timeout=120s
          kubectl rollout status deploy/persona-runtime  --timeout=120s
          kubectl rollout status deploy/viral-engine     --timeout=120s
          # Check revenue deployment if it exists (with release prefix)
          kubectl get deploy/threads-revenue >/dev/null 2>&1 && kubectl rollout status deploy/threads-revenue --timeout=120s || echo "Revenue deployment not found (may be disabled)"
          kubectl rollout status sts/qdrant              --timeout=120s

      # ————————————————————————————————
      # 4.  Set-up Python & cache venv
      # ————————————————————————————————
      - name: Set up Python ${{ matrix.python }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r services/orchestrator/requirements.txt
          pip install -r services/celery_worker/requirements.txt
          pip install -r services/persona_runtime/requirements.txt
          pip install -r services/fake_threads/requirements.txt
          pip install -r services/viral_engine/requirements.txt
          # Install revenue requirements only if the service exists
          if [ -f "services/revenue/requirements.txt" ]; then
            pip install -r services/revenue/requirements.txt
          fi
          pip install -r tests/requirements.txt
          # Uninstall langsmith to prevent pytest plugin loading issues
          pip uninstall -y langsmith || true
          # Create a more complete mock langsmith module
          mkdir -p langsmith
          cat > langsmith/__init__.py << 'EOF'
          class RunTree:
              def __init__(self, *args, **kwargs):
                  pass
          EOF
          cat > langsmith/run_helpers.py << 'EOF'
          def get_tracing_context():
              return None
          EOF
          cat > langsmith/utils.py << 'EOF'
          # Mock utils module
          EOF
          cat > langsmith/schemas.py << 'EOF'
          from enum import Enum
          
          class RunTypeEnum(str, Enum):
              chain = "chain"
              llm = "llm"
              tool = "tool"
              retriever = "retriever"
              embedding = "embedding"
              prompt = "prompt"
              parser = "parser"
          EOF

      # ————————————————————————————————
      # 5.  Run unit + e2e suites
      # ————————————————————————————————
      - name: pytest (unit + e2e)
        id: tests
        env:
          # Disable all langchain tracing to avoid langsmith issues
          LANGCHAIN_TRACING_V2: "false"
          LANGSMITH_TRACING: "false"
          LANGCHAIN_TRACING: "false"
          LANGCHAIN_CALLBACKS_MANAGER: "false"
        run: |
          # ➊ start port-forwards in the background
          kubectl port-forward svc/orchestrator 8080:8080   & PF_ORCH=$!
          kubectl port-forward svc/fake-threads 9009:9009   & PF_THREADS=$!
          kubectl port-forward svc/qdrant 6333:6333         & PF_QDRANT=$!
          kubectl port-forward svc/postgres 15432:5432      & PF_POSTGRES=$!
          # give the tunnel a moment
          sleep 5

          # ➋ run the test suite (tests already hit localhost:8080)
          export PYTHONPATH=$PWD:$PYTHONPATH
          pytest -q

          # ➌ tidy up
          kill $PF_ORCH $PF_THREADS $PF_QDRANT $PF_POSTGRES

      # ─── EXTRA: show Celery-worker logs if the tests failed ──────────────
      - name: Dump worker logs if pytest failed
        if: failure()
        run: |
          echo "⚠️  pytest failed, printing celery-worker logs…"
          kubectl logs deploy/celery-worker --tail=150 || true

      # ————————————————————————————————
      # 6.  Render Mermaid diagram
      # ————————————————————————————————
      - name: Render infra diagram
        run: |
          npm install -g @mermaid-js/mermaid-cli@^10
          mmdc -i docs/infra.mmd -o docs/infra.svg

      # ————————————————————————————————
      # 7.  Upload infra.svg artifact
      # ————————————————————————————————
      - name: Upload infra diagram
        uses: actions/upload-artifact@v4
        with:
          name: infra-svg
          path: docs/infra.svg
