name: Comprehensive Achievement Collector

on:
  pull_request:
    types: [closed]

jobs:
  collect-achievement-data:
    if: github.event.pull_request.merged == true
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for comprehensive analysis
          
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          
      - name: Setup Node.js (for frontend analysis)
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          
      - name: Install dependencies
        run: |
          pip install -r services/achievement_collector/requirements.txt
          
          # Install code analysis tools
          pip install radon bandit mypy pylint coverage pytest-cov
          npm install -g jshint complexity-report
          
      - name: Comprehensive PR Analysis
        id: pr_analysis
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'EOF'
          import json
          import os
          import sys
          
          # Add project to path
          sys.path.insert(0, os.getcwd())
          
          from services.achievement_collector.services.comprehensive_pr_analyzer import ComprehensivePRAnalyzer
          
          # Initialize analyzer
          analyzer = ComprehensivePRAnalyzer()
          
          # Analyze PR
          pr_data = ${{ toJson(github.event.pull_request) }}
          base_sha = "${{ github.event.pull_request.base.sha }}"
          head_sha = "${{ github.event.pull_request.head.sha }}"
          
          analysis = await analyzer.analyze_pr(pr_data, base_sha, head_sha)
          
          # Save comprehensive analysis
          with open('comprehensive_analysis.json', 'w') as f:
              json.dump(analysis, f, indent=2)
          
          print(f"Analysis complete: {len(analysis)} categories analyzed")
          
          # Output key metrics for GitHub Actions
          print(f"::set-output name=overall_impact::{analysis['composite_scores']['overall_impact']}")
          print(f"::set-output name=languages::{','.join(analysis['code_metrics']['languages'].keys())}")
          print(f"::set-output name=has_performance_improvements::{bool(analysis['performance_metrics'])}")
          print(f"::set-output name=has_business_impact::{bool(analysis['business_metrics']['financial_impact'])}")
          EOF
          
      - name: Run Performance Benchmarks
        if: contains(steps.pr_analysis.outputs.languages, 'python')
        continue-on-error: true
        run: |
          # Run performance comparison between base and head
          python3 << 'EOF'
          import subprocess
          import json
          import time
          
          benchmarks = {}
          
          # Checkout base branch
          subprocess.run(["git", "checkout", "${{ github.event.pull_request.base.sha }}"])
          
          # Run base benchmarks (mock for now)
          start = time.time()
          # Would run: pytest benchmarks/ --benchmark-only
          base_time = time.time() - start
          benchmarks["base"] = {"execution_time": base_time}
          
          # Checkout head branch
          subprocess.run(["git", "checkout", "${{ github.event.pull_request.head.sha }}"])
          
          # Run head benchmarks
          start = time.time()
          # Would run: pytest benchmarks/ --benchmark-only
          head_time = time.time() - start
          benchmarks["head"] = {"execution_time": head_time}
          
          # Calculate improvement
          if base_time > 0:
              improvement = ((base_time - head_time) / base_time) * 100
              benchmarks["improvement_percentage"] = improvement
          
          with open('benchmark_results.json', 'w') as f:
              json.dump(benchmarks, f)
          EOF
          
      - name: Code Quality Analysis
        continue-on-error: true
        run: |
          python3 << 'EOF'
          import subprocess
          import json
          
          quality_metrics = {
              "complexity": {},
              "maintainability": {},
              "security": {},
              "test_coverage": {}
          }
          
          # Cyclomatic complexity
          try:
              result = subprocess.run(
                  ["radon", "cc", ".", "-j"],
                  capture_output=True,
                  text=True
              )
              if result.returncode == 0:
                  complexity_data = json.loads(result.stdout)
                  # Process complexity data
                  quality_metrics["complexity"] = {
                      "average": 5.2,  # Mock
                      "max": 12
                  }
          except:
              pass
          
          # Security scan
          try:
              result = subprocess.run(
                  ["bandit", "-r", ".", "-f", "json"],
                  capture_output=True,
                  text=True
              )
              if result.returncode == 0:
                  security_data = json.loads(result.stdout)
                  quality_metrics["security"] = {
                      "issues_found": len(security_data.get("results", [])),
                      "severity_high": 0,
                      "severity_medium": 0
                  }
          except:
              pass
          
          # Test coverage (if tests exist)
          try:
              subprocess.run(["coverage", "run", "-m", "pytest"], check=False)
              result = subprocess.run(
                  ["coverage", "json"],
                  capture_output=True,
                  text=True
              )
              if result.returncode == 0:
                  coverage_data = json.loads(result.stdout)
                  quality_metrics["test_coverage"] = {
                      "line_rate": coverage_data.get("totals", {}).get("percent_covered", 0),
                      "branch_rate": coverage_data.get("totals", {}).get("percent_covered_branches", 0)
                  }
          except:
              pass
          
          with open('quality_metrics.json', 'w') as f:
              json.dump(quality_metrics, f)
          EOF
          
      - name: Extract Visual Evidence
        if: steps.pr_analysis.outputs.has_ui_changes == 'true'
        continue-on-error: true
        run: |
          # Take screenshots of UI changes
          # This would run visual regression tests and capture screenshots
          mkdir -p evidence/screenshots
          echo "Visual evidence collection would happen here"
          
      - name: Generate Architecture Diagrams
        continue-on-error: true
        run: |
          # Generate architecture diagrams from code
          python3 << 'EOF'
          # Would use tools like py2puml or similar
          # For now, just create placeholder
          import os
          os.makedirs('evidence/diagrams', exist_ok=True)
          
          with open('evidence/diagrams/architecture.md', 'w') as f:
              f.write("# Architecture Diagram\nWould be generated from code structure")
          EOF
          
      - name: Generate Achievement Stories
        id: story_generation
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python3 << 'EOF'
          import json
          import asyncio
          from services.achievement_collector.services.story_generator import StoryGenerator
          
          # Load analysis
          with open('comprehensive_analysis.json', 'r') as f:
              analysis = json.load(f)
          
          # Initialize generator
          generator = StoryGenerator()
          
          # Generate stories based on analysis
          stories = {}
          
          # Always try to generate a technical story
          stories['technical'] = await generator.generate_technical_story(analysis)
          
          # Generate other stories if relevant
          if analysis.get('performance_metrics'):
              stories['performance'] = await generator.generate_performance_story(
                  ${{ toJson(github.event.pull_request) }},
                  analysis['performance_metrics'],
                  analysis['code_metrics']
              )
          
          if analysis.get('business_metrics', {}).get('financial_impact'):
              stories['business'] = await generator.generate_business_story(
                  ${{ toJson(github.event.pull_request) }},
                  analysis['business_metrics']
              )
          
          if analysis.get('team_metrics', {}).get('collaboration', {}).get('reviewers_count', 0) > 2:
              stories['leadership'] = await generator.generate_leadership_story(
                  ${{ toJson(github.event.pull_request) }},
                  {}  # Would include review data
              )
          
          if analysis.get('innovation_metrics', {}).get('technical_innovation'):
              stories['innovation'] = await generator.generate_innovation_story(analysis)
          
          # Save stories
          with open('generated_stories.json', 'w') as f:
              json.dump(stories, f, indent=2)
          
          print(f"Generated {len(stories)} story types")
          EOF
          
      - name: Prepare Multi-Platform Content
        run: |
          python3 << 'EOF'
          import json
          import asyncio
          from services.achievement_collector.publishers.multi_platform_publisher import MultiPlatformOrchestrator
          
          # Load all data
          with open('comprehensive_analysis.json', 'r') as f:
              analysis = json.load(f)
              
          with open('generated_stories.json', 'r') as f:
              stories = json.load(f)
          
          # Create achievement object
          achievement = {
              "id": "${{ github.event.pull_request.number }}",
              "metadata": {
                  "pr_number": ${{ github.event.pull_request.number }},
                  "title": "${{ github.event.pull_request.title }}",
                  "description": "${{ github.event.pull_request.body }}",
                  "merged_at": "${{ github.event.pull_request.merged_at }}",
                  "author": "${{ github.event.pull_request.user.login }}",
                  "pr_url": "${{ github.event.pull_request.html_url }}"
              },
              "stories": stories,
              **analysis,  # Include all analysis data
              "evidence": {
                  "screenshots": [],  # Would be populated from visual tests
                  "performance_graphs": [],  # Would be generated from benchmarks
                  "architecture_diagrams": []  # Would be generated from code
              }
          }
          
          # Prepare content for all platforms
          orchestrator = MultiPlatformOrchestrator()
          prepared_content = await orchestrator.prepare_all_content(achievement)
          
          # Save prepared content
          with open('prepared_platform_content.json', 'w') as f:
              json.dump(prepared_content, f, indent=2)
          
          print(f"Content prepared for {len(prepared_content)} platforms")
          
          # Generate preview of each platform's content
          for platform, content in prepared_content.items():
              print(f"\n{platform.upper()} Preview:")
              if platform == "linkedin":
                  print(content.get("text", "")[:200] + "...")
              elif platform == "twitter":
                  print(f"Thread with {len(content.get('thread', []))} tweets")
              elif platform == "devto":
                  print(f"Article: {content.get('title')}")
              # ... etc
          EOF
          
      - name: Store in Achievement Database
        env:
          DATABASE_URL: ${{ secrets.ACHIEVEMENT_DB_URL }}
        run: |
          python3 << 'EOF'
          import json
          import asyncio
          from datetime import datetime
          from sqlalchemy import create_engine
          from sqlalchemy.orm import sessionmaker
          
          from services.achievement_collector.db.models import Base, Achievement, PRAchievement
          
          # Load all collected data
          with open('comprehensive_analysis.json', 'r') as f:
              analysis = json.load(f)
              
          with open('generated_stories.json', 'r') as f:
              stories = json.load(f)
              
          with open('prepared_platform_content.json', 'r') as f:
              platform_content = json.load(f)
          
          # Additional metrics
          quality_metrics = {}
          try:
              with open('quality_metrics.json', 'r') as f:
                  quality_metrics = json.load(f)
          except:
              pass
              
          benchmark_results = {}
          try:
              with open('benchmark_results.json', 'r') as f:
                  benchmark_results = json.load(f)
          except:
              pass
          
          # Create database session
          engine = create_engine(os.getenv('DATABASE_URL'))
          Base.metadata.create_all(bind=engine)
          SessionLocal = sessionmaker(bind=engine)
          db = SessionLocal()
          
          try:
              # Create main achievement record
              achievement = Achievement(
                  title=f"PR #{pr_data['number']}: {pr_data['title']}",
                  description=pr_data.get('body', '')[:1000],
                  category=self._determine_category(analysis),
                  started_at=datetime.fromisoformat(pr_data['created_at'].replace('Z', '+00:00')),
                  completed_at=datetime.fromisoformat(pr_data['merged_at'].replace('Z', '+00:00')),
                  source_type='github_pr',
                  source_id=f"PR-{pr_data['number']}",
                  source_url=pr_data['html_url'],
                  impact_score=analysis['composite_scores']['overall_impact'],
                  complexity_score=analysis['composite_scores']['technical_excellence'],
                  portfolio_ready=analysis['composite_scores']['overall_impact'] > 70,
                  tags=self._extract_tags(analysis),
                  skills_demonstrated=self._extract_skills(analysis),
                  evidence={
                      'analysis': analysis,
                      'stories': stories,
                      'platform_content': platform_content
                  },
                  metrics_before={},
                  metrics_after={
                      **analysis.get('performance_metrics', {}),
                      **analysis.get('business_metrics', {}),
                      **quality_metrics,
                      **benchmark_results
                  }
              )
              
              db.add(achievement)
              db.commit()
              
              # Create detailed PR achievement record
              pr_achievement = PRAchievement(
                  achievement_id=achievement.id,
                  pr_number=pr_data['number'],
                  title=pr_data['title'],
                  description=pr_data.get('body', ''),
                  merge_timestamp=datetime.fromisoformat(pr_data['merged_at'].replace('Z', '+00:00')),
                  author=pr_data['user']['login'],
                  reviewers=[r['login'] for r in pr_data.get('requested_reviewers', [])],
                  code_analysis=analysis['code_metrics'],
                  impact_analysis={
                      'performance': analysis.get('performance_metrics', {}),
                      'business': analysis.get('business_metrics', {}),
                      'quality': analysis.get('quality_metrics', {})
                  },
                  stories=stories,
                  ci_metrics={
                      'workflow_run_id': '${{ github.run_id }}',
                      'workflow_run_url': '${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}'
                  },
                  performance_metrics=analysis.get('performance_metrics', {}),
                  quality_metrics=quality_metrics,
                  posting_metadata={
                      'linkedin': {'prepared': True, 'posted': False},
                      'twitter': {'prepared': True, 'posted': False},
                      'devto': {'prepared': True, 'posted': False},
                      'github': {'prepared': True, 'posted': False},
                      'portfolio': {'prepared': True, 'posted': False}
                  }
              )
              
              db.add(pr_achievement)
              db.commit()
              
              print(f"✅ Achievement {achievement.id} stored successfully")
              print(f"   Impact Score: {achievement.impact_score:.1f}/100")
              print(f"   Stories Generated: {len(stories)}")
              print(f"   Platforms Ready: {len(platform_content)}")
              
          finally:
              db.close()
          EOF
          
      - name: Post Achievement Summary
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            // Load analysis results
            const analysis = JSON.parse(fs.readFileSync('comprehensive_analysis.json', 'utf8'));
            const stories = JSON.parse(fs.readFileSync('generated_stories.json', 'utf8'));
            const platformContent = JSON.parse(fs.readFileSync('prepared_platform_content.json', 'utf8'));
            
            // Create comprehensive summary
            let comment = '## 🏆 Comprehensive Achievement Analysis Complete!\n\n';
            
            // Overall scores
            comment += '### 📊 Impact Scores\n';
            const scores = analysis.composite_scores;
            comment += `- **Overall Impact**: ${scores.overall_impact.toFixed(1)}/100\n`;
            comment += `- **Technical Excellence**: ${scores.technical_excellence.toFixed(1)}/100\n`;
            comment += `- **Business Value**: ${scores.business_value.toFixed(1)}/100\n`;
            comment += `- **Innovation Index**: ${scores.innovation_index.toFixed(1)}/100\n\n`;
            
            // Key metrics
            comment += '### 🎯 Key Metrics Captured\n';
            
            // Performance metrics
            const perfMetrics = analysis.performance_metrics;
            if (perfMetrics.latency_changes?.reported) {
              comment += `- **Performance**: ${perfMetrics.latency_changes.reported.improvement_percentage.toFixed(0)}% latency improvement\n`;
            }
            
            // Business metrics
            const bizMetrics = analysis.business_metrics;
            if (bizMetrics.financial_impact?.cost_savings) {
              comment += `- **Cost Savings**: $${bizMetrics.financial_impact.cost_savings.toLocaleString()}\n`;
            }
            
            // Code metrics
            const codeMetrics = analysis.code_metrics;
            comment += `- **Code Changes**: ${codeMetrics.total_lines_added} additions, ${codeMetrics.total_lines_deleted} deletions across ${codeMetrics.files_changed} files\n`;
            comment += `- **Languages**: ${Object.keys(codeMetrics.languages).join(', ')}\n\n`;
            
            // Stories generated
            comment += `### 📝 Achievement Stories Generated (${Object.keys(stories).length})\n`;
            for (const [type, story] of Object.entries(stories)) {
              if (story.summary) {
                comment += `- **${type}**: ${story.summary}\n`;
              }
            }
            
            // Platform readiness
            comment += `\n### 🚀 Ready for Multi-Platform Publishing\n`;
            comment += `Content prepared for: ${Object.keys(platformContent).join(', ')}\n\n`;
            
            // AI insights
            if (analysis.ai_insights?.key_achievements) {
              comment += '### 🤖 AI Insights\n';
              for (const insight of analysis.ai_insights.key_achievements.slice(0, 3)) {
                comment += `- ${insight}\n`;
              }
            }
            
            comment += '\n---\n';
            comment += '*This comprehensive analysis is stored and ready for portfolio generation and multi-platform publishing.*';
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
            
      - name: Upload Analysis Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: achievement-analysis-${{ github.event.pull_request.number }}
          path: |
            comprehensive_analysis.json
            generated_stories.json
            prepared_platform_content.json
            quality_metrics.json
            benchmark_results.json
          retention-days: 90