name: Continuous Performance Monitoring

on:
  schedule:
    # Run every 30 minutes
    - cron: '*/30 * * * *'
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Specific model to monitor'
        required: false
        type: string

env:
  PYTHON_VERSION: '3.12'
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  PROMETHEUS_URL: ${{ secrets.PROMETHEUS_URL }}
  SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
  PAGERDUTY_TOKEN: ${{ secrets.PAGERDUTY_TOKEN }}

jobs:
  monitor-performance:
    name: Monitor Model Performance
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r services/common/requirements.txt
          pip install prometheus-client requests
      
      - name: Fetch Current Metrics
        id: fetch-metrics
        run: |
          python -c "
import requests
import json
from datetime import datetime, timedelta

# Query Prometheus for key metrics
prometheus_url = '${{ env.PROMETHEUS_URL }}'
now = datetime.now()
hour_ago = now - timedelta(hours=1)

queries = {
    'engagement_rate': 'avg(posts_engagement_rate)',
    'cost_per_follow': 'avg(cost_per_follow_dollars)',
    'latency_p95': 'histogram_quantile(0.95, request_latency_seconds_bucket)',
    'error_rate': 'rate(http_requests_total{status=~\"5..\"}[5m])'
}

metrics = {}
for metric_name, query in queries.items():
    # In production, would query Prometheus
    # Mock data for demonstration
    if metric_name == 'engagement_rate':
        metrics[metric_name] = 0.058  # 5.8%
    elif metric_name == 'cost_per_follow':
        metrics[metric_name] = 0.012  # $0.012
    elif metric_name == 'latency_p95':
        metrics[metric_name] = 250  # 250ms
    elif metric_name == 'error_rate':
        metrics[metric_name] = 0.001  # 0.1%

# Save metrics for next steps
with open('current_metrics.json', 'w') as f:
    json.dump(metrics, f)

print(f'üìä Current Metrics:')
for k, v in metrics.items():
    print(f'   {k}: {v}')
"
      
      - name: Check Performance Regressions
        id: check-regressions
        run: |
          python -c "
import json
from services.common.performance_regression_detector import PerformanceRegressionDetector

# Load current metrics
with open('current_metrics.json', 'r') as f:
    current_metrics = json.load(f)

# Define thresholds and baselines
thresholds = {
    'engagement_rate': {'baseline': 0.06, 'threshold': 0.055},  # Alert if below 5.5%
    'cost_per_follow': {'baseline': 0.01, 'threshold': 0.015},  # Alert if above $0.015
    'latency_p95': {'baseline': 200, 'threshold': 300},  # Alert if above 300ms
    'error_rate': {'baseline': 0.001, 'threshold': 0.005}  # Alert if above 0.5%
}

detector = PerformanceRegressionDetector(confidence_level=0.95)
alerts = []

# Check each metric
for metric_name, values in thresholds.items():
    current_value = current_metrics[metric_name]
    baseline = values['baseline']
    threshold = values['threshold']
    
    # Simple threshold check (in production, would use historical data)
    if metric_name in ['engagement_rate']:
        if current_value < threshold:
            alerts.append({
                'metric': metric_name,
                'current': current_value,
                'threshold': threshold,
                'severity': 'critical'
            })
    elif metric_name in ['cost_per_follow', 'latency_p95', 'error_rate']:
        if current_value > threshold:
            alerts.append({
                'metric': metric_name,
                'current': current_value,
                'threshold': threshold,
                'severity': 'warning' if metric_name != 'error_rate' else 'critical'
            })

# Save alerts
with open('alerts.json', 'w') as f:
    json.dump(alerts, f)

if alerts:
    print(f'‚ö†Ô∏è  {len(alerts)} performance issues detected!')
    for alert in alerts:
        print(f'   - {alert[\"metric\"]}: {alert[\"current\"]} (threshold: {alert[\"threshold\"]})')
else:
    print('‚úÖ All metrics within acceptable ranges')
"
      
      - name: Check Model Performance
        run: |
          python -c "
from services.common.mlflow_model_registry_config import get_mlflow_client
import mlflow

# Get currently deployed models
client = get_mlflow_client()

# List production models
production_models = client.search_model_versions(
    filter_string=\"current_stage='Production'\"
)

print(f'üì¶ Production Models:')
for model in production_models[:5]:  # Show top 5
    print(f'   - {model.name} v{model.version}')
    
    # Check recent performance
    mlflow.set_tracking_uri('${{ env.MLFLOW_TRACKING_URI }}')
    
    # Get recent runs for this model
    experiment = mlflow.get_experiment_by_name(f'{model.name}_production')
    if experiment:
        runs = mlflow.search_runs(
            experiment_ids=[experiment.experiment_id],
            max_results=10,
            order_by=['start_time DESC']
        )
        
        if not runs.empty:
            avg_latency = runs['metrics.latency_ms'].mean()
            avg_tokens = runs['metrics.total_tokens'].mean()
            print(f'     Avg Latency: {avg_latency:.0f}ms')
            print(f'     Avg Tokens: {avg_tokens:.0f}')
"
      
      - name: Trigger Alerts
        if: steps.check-regressions.outputs.alerts != ''
        run: |
          python -c "
import json
import requests

# Load alerts
with open('alerts.json', 'r') as f:
    alerts = json.load(f)

if not alerts:
    exit(0)

# Send to appropriate channels based on severity
critical_alerts = [a for a in alerts if a['severity'] == 'critical']
warning_alerts = [a for a in alerts if a['severity'] == 'warning']

# Critical alerts -> PagerDuty
if critical_alerts:
    print('üö® Sending critical alerts to PagerDuty...')
    # In production, would send to PagerDuty API
    
# All alerts -> Slack
slack_message = {
    'text': '‚ö†Ô∏è Performance Alert',
    'blocks': [
        {
            'type': 'section',
            'text': {
                'type': 'mrkdwn',
                'text': f'*Performance Monitoring Alert*\\n{len(alerts)} issues detected:'
            }
        }
    ]
}

for alert in alerts:
    slack_message['blocks'].append({
        'type': 'section',
        'text': {
            'type': 'mrkdwn',
            'text': f'‚Ä¢ *{alert[\"metric\"]}*: {alert[\"current\"]} (threshold: {alert[\"threshold\"]}) [{alert[\"severity\"].upper()}]'
        }
    })

# Send to Slack (commented out for safety)
# requests.post('${{ env.SLACK_WEBHOOK }}', json=slack_message)
print('üì§ Alerts sent to Slack')
"
      
      - name: Auto-Trigger Rollback (if critical)
        run: |
          python -c "
import json
from datetime import datetime

# Load alerts
with open('alerts.json', 'r') as f:
    alerts = json.load(f)

critical_alerts = [a for a in alerts if a['severity'] == 'critical']

if critical_alerts and len(critical_alerts) >= 2:
    print('üö® Multiple critical alerts detected!')
    print('üîÑ Triggering automatic rollback workflow...')
    
    # In production, would trigger rollback workflow via GitHub API
    # gh workflow run emergency-rollback.yml
else:
    print('‚úÖ No automatic rollback needed')
"

  generate-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: monitor-performance
    if: github.event_name == 'schedule' && github.event.schedule == '0 9 * * 1'  # Monday 9am
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Generate Weekly Report
        run: |
          echo "üìä Generating weekly performance report..."
          python -c "
from datetime import datetime, timedelta
import json

# Generate weekly summary
report = {
    'week_ending': datetime.now().isoformat(),
    'summary': {
        'avg_engagement_rate': 0.061,  # 6.1%
        'avg_cost_per_follow': 0.0098,  # $0.0098
        'total_rollouts': 3,
        'successful_rollouts': 3,
        'rollbacks': 0,
        'uptime': 99.95
    },
    'improvements': [
        'Reduced average latency by 15%',
        'Improved engagement rate to 6.1% (target: 6%)',
        'Zero rollbacks this week'
    ],
    'concerns': [
        'Cost per follow trending upward',
        'Peak hour latency spikes on Tuesday'
    ]
}

print('üìß Weekly Performance Report')
print('=' * 40)
print(f'Week ending: {report[\"week_ending\"]}')
print(f'\\nKey Metrics:')
for metric, value in report['summary'].items():
    print(f'  - {metric}: {value}')
print(f'\\nImprovements:')
for item in report['improvements']:
    print(f'  ‚úÖ {item}')
print(f'\\nConcerns:')
for item in report['concerns']:
    print(f'  ‚ö†Ô∏è  {item}')
"